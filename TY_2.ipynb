{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08290e3",
   "metadata": {},
   "source": [
    "# Resample dff data to have the same speed distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "directory_path = r'D:\\2P_DATA\\ChAT_MEC_switch_analysis_240822\\Speed_min\\Switch_timing'\n",
    "\n",
    "# Get all CSV files in the directory\n",
    "csv_files = glob.glob(os.path.join(directory_path, '*.csv'))\n",
    "\n",
    "# Load each CSV file into a DataFrame and store in a list\n",
    "switch = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "print(switch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f44922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "directory_path = r'D:\\2P_DATA\\ChAT_MEC_switch_analysis_240822\\Speed_min\\df_downsample'\n",
    "\n",
    "# Get all CSV files in the directory\n",
    "csv_files = glob.glob(os.path.join(directory_path, '*.csv'))\n",
    "\n",
    "# Load each CSV file into a DataFrame and store in a list\n",
    "df_downsample = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "print(df_downsample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9dde93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the directory containing the .npy files\n",
    "directory = r\"D:\\2P_DATA\\ChAT_MEC_switch_analysis_240822\\Novelty_sec\\dff_f_man_from_each_dff\"\n",
    "\n",
    "# List all .npy files in the directory\n",
    "file_list = [file for file in os.listdir(directory) if file.endswith('.npy')]\n",
    "\n",
    "# Load all .npy files into a dictionary with filenames as keys\n",
    "data_dict = {file: np.load(os.path.join(directory, file)) for file in file_list}\n",
    "\n",
    "# Now, data_dict contains all the loaded arrays with filenames as keys\n",
    "for filename, data in data_dict.items():\n",
    "    print(f\"Filename: {filename}\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0740ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the arrays in the dictionary into a list of numpy arrays\n",
    "dff_f_mean = [data for data in data_dict.values()]\n",
    "\n",
    "# Optionally, you can inspect the result\n",
    "print(dff_f_mean)\n",
    "print(\"Number of arrays:\", len(dff_f_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f18a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "from matplotlib import mlab\n",
    "from scipy.signal import detrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f014eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funtion\n",
    "def bwlabel(data):\n",
    "    count=0\n",
    "    label= np.zeros((len(data)))\n",
    "    if data[0]!=0:\n",
    "        label[0]=1\n",
    "\n",
    "    for i in range(1,len(data)):\n",
    "        if (data[i]!=0) & (data[i-1]!=0):\n",
    "            label[i]=count\n",
    "        elif (data[i]!=0) & (data[i-1]==0):\n",
    "            count=count+1\n",
    "            label[i]=count\n",
    "\n",
    "    return label\n",
    "\n",
    "def double_thresh(data, lower_thresh,upper_thresh):\n",
    "    state = np.zeros((len(data)))\n",
    "    if data[0]<lower_thresh:\n",
    "        state[0]=0\n",
    "    else:\n",
    "        state[0]=1\n",
    "    \n",
    "    for i in range(1,len(data)):\n",
    "        state[i]=state[i-1]\n",
    "        if state[i-1]==0:\n",
    "            if data[i]>upper_thresh:\n",
    "                state[i]=1\n",
    "        else:\n",
    "            if data[i]<lower_thresh:\n",
    "                state[i]=0\n",
    "    \n",
    "    return state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a63939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data by speed\n",
    "\n",
    "def filter_running(speed, lower_thresh, upper_thresh, sit_thresh, run_thresh, max_value):\n",
    "    \"\"\"\n",
    "    Filters running data based on thresholds for speed, immobility, and run duration.\n",
    "\n",
    "    Args:\n",
    "    - speed: Speed data (array).\n",
    "    - lower_thresh: Lower threshold for running detection.\n",
    "    - upper_thresh: Upper threshold for running detection.\n",
    "    - sit_thresh: Minimum time (seconds) for immobility to be excluded.\n",
    "    - run_thresh: Minimum time (seconds) for running to be included.\n",
    "    - max_value: (Optional) Exclude data if speed exceeds this value.\n",
    "\n",
    "    Returns:\n",
    "    - running_index: Indices of data points considered as running.\n",
    "    \"\"\"\n",
    "    # Double threshold\n",
    "    run_state = double_thresh(speed, lower_thresh, upper_thresh)\n",
    "    \n",
    "    # Include small periods of immobility if smaller than sit_thresh seconds\n",
    "    run_state_invert = np.where(run_state == 0, 1, 0)\n",
    "    run_state_invert2 = bwlabel(run_state_invert)\n",
    "    for i in range(int(np.max(run_state_invert2))):\n",
    "        if len(run_state_invert2[run_state_invert2 == i]) < 60 * sit_thresh:\n",
    "            run_state = np.where(run_state_invert2 == i, 1, run_state)\n",
    "    \n",
    "    # Exclude small periods of running if smaller than run_thresh seconds\n",
    "    run_state2 = bwlabel(run_state)\n",
    "    for i in range(int(np.max(run_state2))):\n",
    "        if len(run_state2[run_state2 == i]) < 60 * run_thresh:\n",
    "            run_state = np.where(run_state2 == i, 0, run_state)\n",
    "    \n",
    "    # Get running indices\n",
    "    running_index = np.squeeze(np.array([ind for (ind, val) in np.ndenumerate(run_state) if val == 1]))\n",
    "\n",
    "    # Exclude data points where speed exceeds the max_value, if provided\n",
    "    if max_value is not None:\n",
    "        running_index = np.array([idx for idx in running_index if speed[idx] <= max_value])\n",
    "    \n",
    "    return running_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_familiar_all=[] #velocity value\n",
    "filtered_speed_minute_familiar=[] #velocity value\n",
    "filtered_dff_minute_novel=[] #velocity value\n",
    "filtered_dff_minute_Rfamiliar=[] #velocity value\n",
    "filtered_dff_minute_familiar=[] #dff value\n",
    "filtered_dff_minute_novel2=[] #dff value\n",
    "filtered_dff_minute_Rfamiliar2=[] #dff value\n",
    "running_index_familiar_list=[] #index\n",
    "running_index_novel_list=[] #index\n",
    "running_index_Rfamiliar_list=[] #index\n",
    "\n",
    "for i in range(0,len(df_downsample)):\n",
    "    velocity_f = savgol_filter(df_downsample[i].Velocity,50,3)\n",
    "    velocity_f=pd.DataFrame(velocity_f)\n",
    "    velocity_f_positive=velocity_f[0] #velocity_f[0]-velocity_f[0].min()+0.0000001\n",
    "    switch1=switch[i].switch1.iloc[0]\n",
    "    switch2=switch[i].switch2.iloc[0]\n",
    "    # Filter running speed\n",
    "    speed_familiar= np.array(velocity_f_positive[:switch1])\n",
    "    running_index_familiar = filter_running(speed_familiar,lower_thresh=1.0,upper_thresh=1.0,sit_thresh=1,run_thresh=1,max_value=2) \n",
    "    speed_familiar = speed_familiar[running_index_familiar] \n",
    "    speed_novel= np.array(velocity_f_positive[switch1:switch2])#could use\"int(switch1+(switch2-switch1)/2)\" insteatd of \"switch2\"\n",
    "    running_index_novel = filter_running(speed_novel,lower_thresh=0,upper_thresh=0,sit_thresh=1,run_thresh=1,max_value=6)\n",
    "    speed_novel = speed_novel[running_index_novel]\n",
    "    speed_Rfamiliar= np.array(velocity_f_positive[switch2:])\n",
    "    running_index_Rfamiliar = filter_running(speed_Rfamiliar,lower_thresh=0,upper_thresh=0,sit_thresh=1,run_thresh=1,max_value=6)\n",
    "    speed_Rfamiliar = speed_Rfamiliar[running_index_Rfamiliar] \n",
    "    speed_familiar_all=speed_familiar_all+[speed_familiar]\n",
    "    running_index_familiar_list.append(running_index_familiar) #index\n",
    "    running_index_novel_list.append(running_index_novel) #index\n",
    "    running_index_Rfamiliar_list.append(running_index_Rfamiliar) #index\n",
    "    \n",
    "    df0=df_downsample[i][:switch1]\n",
    "    df0=df0.reset_index().drop(columns=\"index\")\n",
    "    df0.Time=df0.Time-df0.Time[0]\n",
    "    C_indices=running_index_familiar\n",
    "    filtered_dff_average_minute_familiar_each=[]\n",
    "    velocity_minute_each=[]\n",
    "    for n in range(1,10+1):\n",
    "        if n == 1:\n",
    "            # Compute the absolute difference between the 'Time' column and 1\n",
    "            abs_diff = np.abs(df0.Time[C_indices] - n*60)\n",
    "            # Find the index of the minimum value in the absolute difference series\n",
    "            X = np.argmin(abs_diff)\n",
    "            filtered_dff_minute=dff_f_mean[i][:switch1][C_indices][:X]\n",
    "            filtered_dff_average_minute_familiar_each.append(filtered_dff_minute)\n",
    "            velocity_minute=velocity_f_positive[:switch1][C_indices][:X]\n",
    "            velocity_minute_each.append(velocity_minute)\n",
    "        else:\n",
    "            abs_diff1 = np.abs(df0.Time[C_indices] - (n-1)*60)\n",
    "            X1 = np.argmin(abs_diff1)\n",
    "            abs_diff2 = np.abs(df0.Time[C_indices] - n*60)\n",
    "            X2 = np.argmin(abs_diff2)\n",
    "            filtered_dff_minute=dff_f_mean[i][:switch1][C_indices][X1:X2]\n",
    "            filtered_dff_average_minute_familiar_each.append(filtered_dff_minute)\n",
    "            velocity_minute=velocity_f_positive[:switch1][C_indices][X1:X2]\n",
    "            velocity_minute_each.append(velocity_minute)\n",
    "    filtered_dff_minute_familiar += [filtered_dff_average_minute_familiar_each]\n",
    "    filtered_speed_minute_familiar += [velocity_minute_each]\n",
    "    \n",
    "    df1=df_downsample[i][switch1:switch2]\n",
    "    df1=df1.reset_index().drop(columns=\"index\")\n",
    "    df1.Time=df1.Time-df1.Time[0]\n",
    "    #velocity_f = savgol_filter(df_downsample[i].Velocity,50,3)\n",
    "    #velocity_f=pd.DataFrame(velocity_f)\n",
    "    #velocity_f_positive=velocity_f[0]-velocity_f[0].min()+0.0000001\n",
    "    velocity_f_positive_novel=velocity_f_positive[switch1:switch2]\n",
    "    velocity_f_positive_novel=velocity_f_positive_novel.reset_index().drop(columns=\"index\")\n",
    "    velocity_f_positive_novel=np.array(velocity_f_positive_novel).ravel()\n",
    "    C_indices=running_index_novel\n",
    "    filtered_dff_average_minute_novel_each=[]\n",
    "    filtered_dff_average_minute_novel_each2=[]\n",
    "    for n in range(1,20+1):\n",
    "        if n == 1:\n",
    "            # Compute the absolute difference between the 'Time' column and 1\n",
    "            abs_diff = np.abs(df1.Time[C_indices] - n*60)\n",
    "            # Find the index of the minimum value in the absolute difference series\n",
    "            X = np.argmin(abs_diff)\n",
    "            filtered_dff_minute=velocity_f_positive_novel[C_indices][:X]\n",
    "            filtered_dff_average_minute_novel_each.append(filtered_dff_minute) \n",
    "            filtered_dff_minute2=dff_f_mean[i][switch1:switch2][C_indices][:X]\n",
    "            filtered_dff_average_minute_novel_each2.append(filtered_dff_minute2) \n",
    "        else:\n",
    "            abs_diff1 = np.abs(df1.Time[C_indices] - (n-1)*60)\n",
    "            X1 = np.argmin(abs_diff1)\n",
    "            abs_diff2 = np.abs(df1.Time[C_indices] - n*60)\n",
    "            X2 = np.argmin(abs_diff2)\n",
    "            filtered_dff_minute=velocity_f_positive_novel[C_indices][X1:X2]\n",
    "            filtered_dff_average_minute_novel_each.append(filtered_dff_minute) \n",
    "            filtered_dff_minute2=dff_f_mean[i][switch1:switch2][C_indices][X1:X2]\n",
    "            filtered_dff_average_minute_novel_each2.append(filtered_dff_minute2) \n",
    "    filtered_dff_minute_novel += [filtered_dff_average_minute_novel_each]\n",
    "    filtered_dff_minute_novel2 += [filtered_dff_average_minute_novel_each2]\n",
    "    \n",
    "    df2=df_downsample[i][switch2:]\n",
    "    df2=df2.reset_index().drop(columns=\"index\")\n",
    "    df2.Time=df2.Time-df2.Time[0]\n",
    "    velocity_f_positive_Rfamiliar=velocity_f_positive[switch2:]\n",
    "    velocity_f_positive_Rfamiliar=velocity_f_positive_Rfamiliar.reset_index().drop(columns=\"index\")\n",
    "    velocity_f_positive_Rfamiliar=np.array(velocity_f_positive_Rfamiliar).ravel()\n",
    "    C_indices=running_index_Rfamiliar\n",
    "    filtered_dff_average_minute_Rfamiliar_each=[]\n",
    "    filtered_dff_average_minute_Rfamiliar_each2=[]\n",
    "    for n in range(1,10+1):\n",
    "        if n == 1:\n",
    "            # Compute the absolute difference between the 'Time' column and 1\n",
    "            abs_diff = np.abs(df2.Time[C_indices] - n*60)\n",
    "            # Find the index of the minimum value in the absolute difference series\n",
    "            X = np.argmin(abs_diff)\n",
    "            filtered_dff_minute=velocity_f_positive_Rfamiliar[C_indices][:X]\n",
    "            filtered_dff_average_minute_Rfamiliar_each.append(filtered_dff_minute) \n",
    "            filtered_dff_minute2=dff_f_mean[i][switch2:][C_indices][:X]\n",
    "            filtered_dff_average_minute_Rfamiliar_each2.append(filtered_dff_minute2) \n",
    "        else:\n",
    "            abs_diff1 = np.abs(df2.Time[C_indices] - (n-1)*60)\n",
    "            X1 = np.argmin(abs_diff1)\n",
    "            abs_diff2 = np.abs(df2.Time[C_indices] - n*60)\n",
    "            X2 = np.argmin(abs_diff2)\n",
    "            filtered_dff_minute=velocity_f_positive_Rfamiliar[C_indices][X1:X2]\n",
    "            filtered_dff_average_minute_Rfamiliar_each.append(filtered_dff_minute)\n",
    "            filtered_dff_minute2=dff_f_mean[i][switch2:][C_indices][X1:X2]\n",
    "            filtered_dff_average_minute_Rfamiliar_each2.append(filtered_dff_minute2) \n",
    "    filtered_dff_minute_Rfamiliar += [filtered_dff_average_minute_Rfamiliar_each]\n",
    "    filtered_dff_minute_Rfamiliar2 += [filtered_dff_average_minute_Rfamiliar_each2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(speed_familiar_all),len(filtered_dff_minute_novel),len(filtered_dff_minute_Rfamiliar)) \n",
    "print(len(filtered_dff_minute_familiar), len(filtered_dff_minute_novel2), len(filtered_dff_minute_Rfamiliar2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ebb86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_dff_minute_novel[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288b8a7",
   "metadata": {},
   "source": [
    "# Resampling dff each min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def distance(a, b):\n",
    "    #return np.abs(a - b)\n",
    "\n",
    "def distance(a, b):\n",
    "    return (a - b) ** 2  # Squared difference for 1D\n",
    "\n",
    "def resample_weighted(A, B, N):\n",
    "    unique_A, counts_A = np.unique(A, return_counts=True)\n",
    "    probabilities = counts_A / np.sum(counts_A)  # Probability of selecting each unique element in A\n",
    "    \n",
    "    newsample = np.zeros(N)  # Resampled points\n",
    "    indices = np.zeros(N, dtype=int)  # Indices of the original B\n",
    "    used_indices = set()  # To track which indices in B have been used\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Step 1: Weighted sampling based on the distribution of A\n",
    "        a = np.random.choice(unique_A, p=probabilities)\n",
    "        \n",
    "        # Step 2: Find the closest point in B that hasn't been used\n",
    "        mindist = np.inf\n",
    "        mindistind = None\n",
    "        for j in range(len(B)):\n",
    "            if j not in used_indices:  # Skip already used indices\n",
    "                d = distance(a, B[j])\n",
    "                if d < mindist:\n",
    "                    mindist = d\n",
    "                    mindistind = j\n",
    "        \n",
    "        if mindistind is not None:\n",
    "            newsample[i] = float(B[mindistind])  # Ensure it's a scalar\n",
    "            indices[i] = mindistind\n",
    "            used_indices.add(mindistind)  # Mark this index as used\n",
    "        else:\n",
    "            newsample[i] = np.nan  # Handle the case where no valid index was found\n",
    "\n",
    "    # Sort the newsample based on indices to align with the original B order\n",
    "    sorted_indices = np.argsort(indices)\n",
    "    aligned_newsample = newsample[sorted_indices]\n",
    "    aligned_indices = indices[sorted_indices]\n",
    "    \n",
    "    return aligned_newsample, aligned_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ef774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample data in env2\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "resampled_filtered_novel_all = []\n",
    "resampled_filtered_novel_indices_all = []\n",
    "\n",
    "for i in range(len(df_downsample)):\n",
    "    resampled_mouse = []\n",
    "    resampled_mouse_indices = []\n",
    "    \n",
    "    for n in range(20):\n",
    "        A = speed_familiar_all[i]\n",
    "        B = np.array(filtered_dff_minute_novel[i][n])\n",
    "        # Ensure B is flattened to a 1D array before resampling\n",
    "        #B = B.flatten()\n",
    "        \n",
    "        # Initialize resampling attempt counter\n",
    "        attempt = 0\n",
    "        p = 0  # Start with p > 0.05 to enter the loop\n",
    "        resampled_filtered_novel = None  # Reset\n",
    "        resampled_filtered_novel_indices = None  # Reset\n",
    "\n",
    "        # Repeat resampling up to 10 times if p < 0.05\n",
    "        while p <= 0.05 and attempt < 1000:\n",
    "            resampled_filtered_novel, resampled_filtered_novel_indices = resample_weighted(A, B, 100)\n",
    "            stat, p = mannwhitneyu(A, resampled_filtered_novel, alternative='two-sided')\n",
    "            attempt += 1\n",
    "\n",
    "        # If p is still < 0.05 after 10 attempts, set results to 0 and break\n",
    "        if p <= 0.05 and attempt == 1000:\n",
    "            resampled_filtered_novel = np.zeros(50)  # Zero array for consistency\n",
    "            resampled_filtered_novel_indices = np.zeros(50)\n",
    "            print(f\"p-value still < 0.05 after 1000 attempts for n={n}, i={i}\")\n",
    "\n",
    "        # Append the results for the current n and i, ensure copying of valid data\n",
    "        resampled_mouse += [np.copy(resampled_filtered_novel)]\n",
    "        resampled_mouse_indices += [np.copy(resampled_filtered_novel_indices)]\n",
    "    \n",
    "    # Append results for each iteration of i\n",
    "    resampled_filtered_novel_all += [resampled_mouse]\n",
    "    resampled_filtered_novel_indices_all += [resampled_mouse_indices]\n",
    "\n",
    "# Final output\n",
    "print(f\"Total length of resampled_filtered_novel_all: {len(resampled_filtered_novel_all)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resampled_filtered_novel_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_resampled_dff_minute_novel = []\n",
    "\n",
    "for i in range(len(df_downsample)):\n",
    "    C_indices = resampled_filtered_novel_indices_all[i]\n",
    "    filtered_resampled_dff_minute_novel_each = []\n",
    "\n",
    "    for n in range(20):\n",
    "        # Check if all values in C_indices[n] are 0\n",
    "        if np.all(C_indices[n] == 0):\n",
    "            filtered_dff_minute = None\n",
    "        else:\n",
    "            filtered_dff_minute = filtered_dff_minute_novel2[i][n][C_indices[n]].mean()\n",
    "        \n",
    "        # Append the result (None or calculated mean)\n",
    "        filtered_resampled_dff_minute_novel_each.append(filtered_dff_minute)\n",
    "\n",
    "    # Add the result for each 'i' iteration\n",
    "    filtered_resampled_dff_minute_novel.append(filtered_resampled_dff_minute_novel_each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_resampled_dff_minute_novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98600449",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_resampled_dff_minute_novel_all = []\n",
    "\n",
    "for i in range(len(df_downsample)):\n",
    "    C_indices = resampled_filtered_novel_indices_all[i]\n",
    "    filtered_resampled_dff_minute_novel_all_each = []\n",
    "\n",
    "    for n in range(20):\n",
    "        # Check if all values in C_indices[n] are 0\n",
    "        if np.all(C_indices[n] == 0):\n",
    "            filtered_dff_minute = None\n",
    "        else:\n",
    "            filtered_dff_minute = filtered_dff_minute_novel2[i][n][C_indices[n]]\n",
    "        \n",
    "        # Append the result (None or calculated mean)\n",
    "        filtered_resampled_dff_minute_novel_all_each.append(filtered_dff_minute)\n",
    "\n",
    "    # Add the result for each 'i' iteration\n",
    "    filtered_resampled_dff_minute_novel_all.append(filtered_resampled_dff_minute_novel_all_each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b90e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_resampled_velocity_novel = []\n",
    "\n",
    "for i in range(len(df_downsample)):\n",
    "    C_indices = resampled_filtered_novel_indices_all[i]\n",
    "    filtered_resampled_velocity_novel_each = []\n",
    "\n",
    "    for n in range(20):\n",
    "        # Check if all values in C_indices[n] are 0\n",
    "        if np.all(C_indices[n] == 0):\n",
    "            filtered_dff_minute = None\n",
    "        else:\n",
    "            filtered_dff_minute = filtered_dff_minute_novel[i][n][C_indices[n]]\n",
    "        \n",
    "        # Append the result (None or calculated mean)\n",
    "        filtered_resampled_velocity_novel_each.append(filtered_dff_minute)\n",
    "\n",
    "    # Add the result for each 'i' iteration\n",
    "    filtered_resampled_velocity_novel.append(filtered_resampled_velocity_novel_each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c78834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample for returned env1 data\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "resampled_filtered_Rfamiliar_all = []\n",
    "resampled_filtered_Rfamiliar_indices_all = []\n",
    "\n",
    "for i in range(len(df_downsample)):\n",
    "    resampled_mouse = []\n",
    "    resampled_mouse_indices = []\n",
    "    \n",
    "    for n in range(10):\n",
    "        A = speed_familiar_all[i]\n",
    "        B = np.array(filtered_dff_minute_Rfamiliar[i][n])\n",
    "        # Ensure B is flattened to a 1D array before resampling\n",
    "        #B = B.flatten()\n",
    "        \n",
    "        # Initialize resampling attempt counter\n",
    "        attempt = 0\n",
    "        p = 0  # Start with p > 0.05 to enter the loop\n",
    "        resampled_filtered_Rfamiliar = None  # Reset\n",
    "        resampled_filtered_Rfamiliar_indices = None  # Reset\n",
    "\n",
    "        # Repeat resampling up to 10 times if p < 0.05\n",
    "        while p <= 0.05 and attempt < 1000:\n",
    "            resampled_filtered_Rfamiliar, resampled_filtered_Rfamiliar_indices = resample_weighted(A, B, 100)\n",
    "            stat, p = mannwhitneyu(A, resampled_filtered_Rfamiliar, alternative='two-sided')\n",
    "            attempt += 1\n",
    "\n",
    "        # If p is still < 0.05 after 10 attempts, set results to 0 and break\n",
    "        if p <= 0.05 and attempt == 1000:\n",
    "            resampled_filtered_Rfamiliar = np.zeros(50)  # Zero array for consistency\n",
    "            resampled_filtered_Rfamiliar_indices = np.zeros(50)\n",
    "            print(f\"p-value still < 0.05 after 1000 attempts for n={n}, i={i}\")\n",
    "\n",
    "        # Append the results for the current n and i, ensure copying of valid data\n",
    "        resampled_mouse += [np.copy(resampled_filtered_Rfamiliar)]\n",
    "        resampled_mouse_indices += [np.copy(resampled_filtered_Rfamiliar_indices)]\n",
    "    \n",
    "    # Append results for each iteration of i\n",
    "    resampled_filtered_Rfamiliar_all += [resampled_mouse]\n",
    "    resampled_filtered_Rfamiliar_indices_all += [resampled_mouse_indices]\n",
    "\n",
    "# Final output\n",
    "print(f\"Total length of resampled_filtered_Rfamiliar_all: {len(resampled_filtered_Rfamiliar_all)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3828e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_resampled_dff_minute_Rfamiliar = []\n",
    "\n",
    "for i in range(len(df_downsample)):\n",
    "    C_indices = resampled_filtered_Rfamiliar_indices_all[i]\n",
    "    filtered_resampled_dff_minute_Rfamiliar_each = []\n",
    "\n",
    "    for n in range(10):\n",
    "        # Check if all values in C_indices[n] are 0\n",
    "        if np.all(C_indices[n] == 0):\n",
    "            filtered_dff_minute = None\n",
    "        else:\n",
    "            filtered_dff_minute = filtered_dff_minute_Rfamiliar2[i][n][C_indices[n]].mean()\n",
    "        \n",
    "        # Append the result (None or calculated mean)\n",
    "        filtered_resampled_dff_minute_Rfamiliar_each.append(filtered_dff_minute)\n",
    "\n",
    "    # Add the result for each 'i' iteration\n",
    "    filtered_resampled_dff_minute_Rfamiliar.append(filtered_resampled_dff_minute_Rfamiliar_each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bffbf0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_resampled_dff_minute_Rfamiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a118f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_resampled_dff_minute_Rfamiliar_all = []\n",
    "\n",
    "for i in range(len(df_downsample)):\n",
    "    C_indices = resampled_filtered_Rfamiliar_indices_all[i]\n",
    "    filtered_resampled_dff_minute_Rfamiliar_all_each = []\n",
    "\n",
    "    for n in range(10):\n",
    "        # Check if all values in C_indices[n] are 0\n",
    "        if np.all(C_indices[n] == 0):\n",
    "            filtered_dff_minute = None\n",
    "        else:\n",
    "            filtered_dff_minute = filtered_dff_minute_Rfamiliar2[i][n][C_indices[n]]\n",
    "        \n",
    "        # Append the result (None or calculated mean)\n",
    "        filtered_resampled_dff_minute_Rfamiliar_all_each.append(filtered_dff_minute)\n",
    "\n",
    "    # Add the result for each 'i' iteration\n",
    "    filtered_resampled_dff_minute_Rfamiliar_all.append(filtered_resampled_dff_minute_Rfamiliar_all_each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4f5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_resampled_velocity_Rfamiliar = []\n",
    "\n",
    "for i in range(len(df_downsample)):\n",
    "    C_indices = resampled_filtered_Rfamiliar_indices_all[i]\n",
    "    filtered_resampled_velocity_Rfamiliar_each = []\n",
    "\n",
    "    for n in range(10):\n",
    "        # Check if all values in C_indices[n] are 0\n",
    "        if np.all(C_indices[n] == 0):\n",
    "            filtered_dff_minute = None\n",
    "        else:\n",
    "            filtered_dff_minute = filtered_dff_minute_Rfamiliar[i][n][C_indices[n]]\n",
    "        \n",
    "        # Append the result (None or calculated mean)\n",
    "        filtered_resampled_velocity_Rfamiliar_each.append(filtered_dff_minute)\n",
    "\n",
    "    # Add the result for each 'i' iteration\n",
    "    filtered_resampled_velocity_Rfamiliar.append(filtered_resampled_velocity_Rfamiliar_each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c8f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_dff_minute_ave_familiar=[]\n",
    "for i in range(len(df_downsample)):\n",
    "    filtered_dff_minute_ave_familiar_each=[]\n",
    "    for n in range(10):\n",
    "        filtered_dff_minute_ave_familiar_each.append(filtered_dff_minute_familiar[i][n].mean())\n",
    "    filtered_dff_minute_ave_familiar+=[filtered_dff_minute_ave_familiar_each]\n",
    "len(filtered_dff_minute_ave_familiar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Dictionary mapping file names to data variables\n",
    "data_to_save = {\n",
    "    \"filtered_dff_minute_familiar\": filtered_dff_minute_familiar,#dff data\n",
    "    \"filtered_dff_minute_ave_familiar\": filtered_dff_minute_ave_familiar,#dff data\n",
    "    \"filtered_dff_minute_novel\": filtered_dff_minute_novel,#velocity data\n",
    "    \"filtered_dff_minute_novel2\": filtered_dff_minute_novel2,#dff data\n",
    "    \"filtered_resampled_dff_minute_novel\": filtered_resampled_dff_minute_novel,#dff data\n",
    "    \"filtered_dff_minute_Rfamiliar\": filtered_dff_minute_Rfamiliar,#velocity data\n",
    "    \"filtered_dff_minute_Rfamiliar2\": filtered_dff_minute_Rfamiliar2,#dff data\n",
    "    \"filtered_resampled_dff_minute_Rfamiliar\": filtered_resampled_dff_minute_Rfamiliar,#dff data\n",
    "    \"filtered_resampled_velocity_novel\":filtered_resampled_velocity_novel,#velocity data\n",
    "    \"filtered_resampled_velocity_Rfamiliar\":filtered_resampled_velocity_Rfamiliar,#velocity data\n",
    "    \"resampled_filtered_novel_all\":resampled_filtered_novel_all,#dff data\n",
    "    \"resampled_filtered_novel_indices_all\": resampled_filtered_novel_indices_all,#indices\n",
    "    \"resampled_filtered_Rfamiliar_all\":resampled_filtered_Rfamiliar_all,#dff data\n",
    "    \"resampled_filtered_Rfamiliar_indices_all\":resampled_filtered_Rfamiliar_indices_all,#indices\n",
    "    \"running_index_familiar_list\":running_index_familiar_list, #index\n",
    "    \"running_index_novel_list\": running_index_novel_list, #index\n",
    "    \"running_index_Rfamiliar_list\":running_index_Rfamiliar_list, #index\n",
    "    \"speed_familiar_all\":speed_familiar_all#velocity data\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "# Loop through the dictionary and save each dataset to its corresponding file\n",
    "for filename, data in data_to_save.items():\n",
    "    with open(filename, \"wb\") as fp:\n",
    "        pickle.dump(data, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b80d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Dictionary to store the loaded data\n",
    "data_loaded = {}\n",
    "\n",
    "# List of file names\n",
    "file_names = [\n",
    "    \"filtered_dff_minute_familiar\",\n",
    "    \"filtered_dff_minute_ave_familiar\",\n",
    "    \"filtered_dff_minute_novel\",\n",
    "    \"filtered_dff_minute_novel2\",\n",
    "    \"filtered_resampled_dff_minute_novel\",\n",
    "    \"filtered_dff_minute_Rfamiliar\",\n",
    "    \"filtered_dff_minute_Rfamiliar2\",\n",
    "    \"filtered_resampled_dff_minute_Rfamiliar\",\n",
    "    \"filtered_resampled_velocity_novel\",\n",
    "    \"filtered_resampled_velocity_Rfamiliar\",\n",
    "    \"resampled_filtered_novel_all\",\n",
    "    \"resampled_filtered_novel_indices_all\",\n",
    "    \"resampled_filtered_Rfamiliar_all\",\n",
    "    \"resampled_filtered_Rfamiliar_indices_all\",\n",
    "    \"running_index_familiar_list\",\n",
    "    \"running_index_novel_list\",\n",
    "    \"running_index_Rfamiliar_list\",\n",
    "    \"speed_familiar_all\"\n",
    "]\n",
    "\n",
    "# Loop through the file names and load each file\n",
    "for filename in file_names:\n",
    "    with open(filename, \"rb\") as fp:\n",
    "        data_loaded[filename] = pickle.load(fp)\n",
    "\n",
    "# access each loaded data with data_loaded[\"filename\"]\n",
    "filtered_dff_minute_familiar = data_loaded[\"filtered_dff_minute_familiar\"]#dff data\n",
    "filtered_dff_minute_ave_familiar = data_loaded[\"filtered_dff_minute_ave_familiar\"]#dff data\n",
    "filtered_dff_minute_novel = data_loaded[\"filtered_dff_minute_novel\"]#velocity data\n",
    "filtered_dff_minute_novel2 = data_loaded[\"filtered_dff_minute_novel2\"]#dff data\n",
    "filtered_resampled_dff_minute_novel = data_loaded[\"filtered_resampled_dff_minute_novel\"]#dff data\n",
    "filtered_dff_minute_Rfamiliar = data_loaded[\"filtered_dff_minute_Rfamiliar\"]#velocity data\n",
    "filtered_dff_minute_Rfamiliar2 = data_loaded[\"filtered_dff_minute_Rfamiliar2\"]#dff data\n",
    "filtered_resampled_dff_minute_Rfamiliar = data_loaded[\"filtered_resampled_dff_minute_Rfamiliar\"]#dff data\n",
    "filtered_resampled_velocity_novel = data_loaded[\"filtered_resampled_velocity_novel\"]#velocity data\n",
    "filtered_resampled_velocity_Rfamiliar = data_loaded[\"filtered_resampled_velocity_Rfamiliar\"]#velocity data\n",
    "resampled_filtered_novel_all = data_loaded[\"resampled_filtered_novel_all\"]#dff data\n",
    "resampled_filtered_novel_indices_all = data_loaded[\"resampled_filtered_novel_indices_all\"]#indices\n",
    "resampled_filtered_Rfamiliar_all = data_loaded[\"resampled_filtered_Rfamiliar_all\"]#dff data\n",
    "resampled_filtered_Rfamiliar_indices_all = data_loaded[\"resampled_filtered_Rfamiliar_indices_all\"]#indices\n",
    "running_index_familiar_list = data_loaded[\"running_index_familiar_list\"] #index\n",
    "running_index_novel_list = data_loaded[\"running_index_novel_list\"] #index\n",
    "running_index_Rfamiliar_list = data_loaded[\"running_index_Rfamiliar_list\"] #index\n",
    "speed_familiar_all = data_loaded[\"speed_familiar_all\"]#velocity data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0c72b",
   "metadata": {},
   "source": [
    "### Inspection of resample by plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample data\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of rows (i) and columns (n) you want\n",
    "rows = len(speed_familiar_all)  # Assuming i represents the number of entries in speed_familiar_all\n",
    "columns = 10  # n is set to 10 based on your description\n",
    "\n",
    "# Create a figure with a grid of subplots\n",
    "fig, axes = plt.subplots(rows, columns, figsize=(columns * 5, rows * 3))  # Adjust figsize as needed\n",
    "\n",
    "# Plot the KDEs in each subplot\n",
    "for i in range(rows):\n",
    "    for n in range(columns):\n",
    "        ax = axes[i, n]  # Get the current subplot\n",
    "        \n",
    "        # Plot the KDEs\n",
    "        sns.kdeplot(speed_familiar_all[i], color='grey', linewidth=2, linestyle='-', label='Familiar', ax=ax)\n",
    "        sns.kdeplot(filtered_resampled_velocity_novel[i][n], color='red', linewidth=2, linestyle='-', label='Novel', ax=ax)\n",
    "        sns.kdeplot(filtered_resampled_velocity_Rfamiliar[i][n], color='black', linewidth=2, linestyle='-', label='Rfamiliar', ax=ax)\n",
    "        \n",
    "        # Customize the subplot appearance\n",
    "        # Remove x-ticks and y-ticks\n",
    "        ax.set_xticks([])  # Remove x-ticks\n",
    "        ax.set_yticks([])  # Remove y-ticks\n",
    "\n",
    "        # Remove axis labels\n",
    "        ax.set_xlabel('')  # Remove x-axis label\n",
    "        ax.set_ylabel('')  # Remove y-axis label\n",
    "        \n",
    "        # Remove top and right spines\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        \n",
    "        # Change border color and thickness\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color('black')\n",
    "            spine.set_linewidth(2)\n",
    "\n",
    "        # Optionally remove legend for cleaner appearance\n",
    "        # ax.legend(loc='upper right', fontsize=10, frameon=False)\n",
    "\n",
    "# Adjust layout so subplots fit nicely\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot with high resolution (DPI)\n",
    "#plt.savefig(\"filtered_resampled_velocity.png\", dpi=600)  # Increase DPI for higher resolution\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ccd228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtered data before resample\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of rows (i) and columns (n) you want\n",
    "rows = len(speed_familiar_all)  # Assuming i represents the number of entries in speed_familiar_all\n",
    "columns = 10  # n is set to 10 based on your description\n",
    "\n",
    "# Create a figure with a grid of subplots\n",
    "fig, axes = plt.subplots(rows, columns, figsize=(columns * 5, rows * 3))  # Adjust figsize as needed\n",
    "\n",
    "# Plot the KDEs in each subplot\n",
    "for i in range(rows):\n",
    "    for n in range(columns):\n",
    "        ax = axes[i, n]  # Get the current subplot\n",
    "        \n",
    "        # Plot the KDEs\n",
    "        sns.kdeplot(speed_familiar_all[i], color='grey', linewidth=2, linestyle='-', label='Familiar', ax=ax)\n",
    "        sns.kdeplot(filtered_dff_minute_novel[i][n], color='red', linewidth=2, linestyle='-', label='Novel', ax=ax)\n",
    "        sns.kdeplot(filtered_dff_minute_Rfamiliar[i][n], color='black', linewidth=2, linestyle='-', label='Rfamiliar', ax=ax)\n",
    "        \n",
    "        # Customize the subplot appearance\n",
    "        # Remove x-ticks and y-ticks\n",
    "        ax.set_xticks([])  # Remove x-ticks\n",
    "        ax.set_yticks([])  # Remove y-ticks\n",
    "\n",
    "        # Remove axis labels\n",
    "        ax.set_xlabel('')  # Remove x-axis label\n",
    "        ax.set_ylabel('')  # Remove y-axis label\n",
    "        \n",
    "        # Remove top and right spines\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        \n",
    "        # Change border color and thickness\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color('black')\n",
    "            spine.set_linewidth(2)\n",
    "\n",
    "        # Optionally remove legend for cleaner appearance\n",
    "        # ax.legend(loc='upper right', fontsize=10, frameon=False)\n",
    "\n",
    "# Adjust layout so subplots fit nicely\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot with high resolution (DPI)\n",
    "#plt.savefig(\"filtered_velocity_sit_thresh1.png\", dpi=600)  # Increase DPI for higher resolution\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_familiar_all=[]\n",
    "velocity_minute_novel=[]\n",
    "velocity_minute_Rfamiliar=[]\n",
    "velocity_minute_familiar=[]\n",
    "velocity_minute_familiar\n",
    "dff_minute_novel=[]\n",
    "dff_minute_Rfamiliar=[]\n",
    "for i in range(0,len(df_downsample)):\n",
    "    velocity_f = savgol_filter(df_downsample[i].Velocity,50,3)\n",
    "    velocity_f=pd.DataFrame(velocity_f)\n",
    "    velocity_f_positive=velocity_f[0]-velocity_f[0].min()+0.0000001\n",
    "    switch1=switch[i].switch1.iloc[0]\n",
    "    switch2=switch[i].switch2.iloc[0]\n",
    "    # Filter running speed\n",
    "    speed_familiar= np.array(velocity_f_positive[:switch1])\n",
    "\n",
    "    speed_novel= np.array(velocity_f_positive[switch1:switch2])#could use\"int(switch1+(switch2-switch1)/2)\" insteatd of \"switch2\"\n",
    "\n",
    "    speed_Rfamiliar= np.array(velocity_f_positive[switch2:])\n",
    "\n",
    "    velocity_familiar_all=velocity_familiar_all+[speed_familiar]\n",
    "    \n",
    "    df0=df_downsample[i][:switch1]\n",
    "    df0=df0.reset_index().drop(columns=\"index\")\n",
    "    df0.Time=df0.Time-df0.Time[0]\n",
    "\n",
    "    velocity_average_minute_familiar_each=[]\n",
    "    for n in range(1,10+1):\n",
    "        if n == 1:\n",
    "            # Compute the absolute difference between the 'Time' column and 1\n",
    "            abs_diff = np.abs(df0.Time - n*60)\n",
    "            # Find the index of the minimum value in the absolute difference series\n",
    "            X = np.argmin(abs_diff)\n",
    "            velocity_minute=dff_f_mean[i][:switch1][:X]\n",
    "            velocity_average_minute_familiar_each.append(velocity_minute) \n",
    "        else:\n",
    "            abs_diff1 = np.abs(df0.Time - (n-1)*60)\n",
    "            X1 = np.argmin(abs_diff1)\n",
    "            abs_diff2 = np.abs(df0.Time - n*60)\n",
    "            X2 = np.argmin(abs_diff2)\n",
    "            velocity_minute=dff_f_mean[i][:switch1][X1:X2]\n",
    "            velocity_average_minute_familiar_each.append(velocity_minute)\n",
    "    velocity_minute_familiar += [velocity_average_minute_familiar_each]\n",
    "    \n",
    "    df1=df_downsample[i][switch1:switch2]\n",
    "    df1=df1.reset_index().drop(columns=\"index\")\n",
    "    df1.Time=df1.Time-df1.Time[0]\n",
    "    #velocity_f = savgol_filter(df_downsample[i].Velocity,50,3)\n",
    "    #velocity_f=pd.DataFrame(velocity_f)\n",
    "    #velocity_f_positive=velocity_f[0]-velocity_f[0].min()+0.0000001\n",
    "    velocity_f_positive_novel=velocity_f_positive[switch1:switch2]\n",
    "    velocity_f_positive_novel=velocity_f_positive_novel.reset_index().drop(columns=\"index\")\n",
    "    velocity_f_positive_novel=np.array(velocity_f_positive_novel).ravel()\n",
    "    \n",
    "    velocity_average_minute_novel_each=[]\n",
    "    dff_average_minute_novel_each=[]\n",
    "    for n in range(1,20+1):\n",
    "        if n == 1:\n",
    "            # Compute the absolute difference between the 'Time' column and 1\n",
    "            abs_diff = np.abs(df1.Time - n*60)\n",
    "            # Find the index of the minimum value in the absolute difference series\n",
    "            X = np.argmin(abs_diff)\n",
    "            velocity_minute=velocity_f_positive_novel[:X]\n",
    "            velocity_average_minute_novel_each.append(velocity_minute) \n",
    "            dff_minute=dff_f_mean[i][switch1:switch2][:X]\n",
    "            dff_average_minute_novel_each.append(dff_minute) \n",
    "        else:\n",
    "            abs_diff1 = np.abs(df1.Time - (n-1)*60)\n",
    "            X1 = np.argmin(abs_diff1)\n",
    "            abs_diff2 = np.abs(df1.Time - n*60)\n",
    "            X2 = np.argmin(abs_diff2)\n",
    "            velocity_minute=velocity_f_positive_novel[X1:X2]\n",
    "            velocity_average_minute_novel_each.append(velocity_minute) \n",
    "            dff_minute=dff_f_mean[i][switch1:switch2][X1:X2]\n",
    "            dff_average_minute_novel_each.append(dff_minute) \n",
    "    velocity_minute_novel += [velocity_average_minute_novel_each]\n",
    "    dff_minute_novel += [dff_average_minute_novel_each]\n",
    "    \n",
    "    df2=df_downsample[i][switch2:]\n",
    "    df2=df2.reset_index().drop(columns=\"index\")\n",
    "    df2.Time=df2.Time-df2.Time[0]\n",
    "    velocity_f_positive_Rfamiliar=velocity_f_positive[switch2:]\n",
    "    velocity_f_positive_Rfamiliar=velocity_f_positive_Rfamiliar.reset_index().drop(columns=\"index\")\n",
    "    velocity_f_positive_Rfamiliar=np.array(velocity_f_positive_Rfamiliar).ravel()\n",
    "   \n",
    "    velocity_average_minute_Rfamiliar_each=[]\n",
    "    dff_average_minute_Rfamiliar_each=[]\n",
    "    for n in range(1,10+1):\n",
    "        if n == 1:\n",
    "            # Compute the absolute difference between the 'Time' column and 1\n",
    "            abs_diff = np.abs(df2.Time - n*60)\n",
    "            # Find the index of the minimum value in the absolute difference series\n",
    "            X = np.argmin(abs_diff)\n",
    "            velocity_minute=velocity_f_positive_Rfamiliar[:X]\n",
    "            velocity_average_minute_Rfamiliar_each.append(velocity_minute) \n",
    "            dff_minute=dff_f_mean[i][switch2:][:X]\n",
    "            dff_average_minute_Rfamiliar_each.append(dff_minute) \n",
    "        else:\n",
    "            abs_diff1 = np.abs(df2.Time - (n-1)*60)\n",
    "            X1 = np.argmin(abs_diff1)\n",
    "            abs_diff2 = np.abs(df2.Time - n*60)\n",
    "            X2 = np.argmin(abs_diff2)\n",
    "            velocity_minute=velocity_f_positive_Rfamiliar[X1:X2]\n",
    "            velocity_average_minute_Rfamiliar_each.append(velocity_minute)\n",
    "            dff_minute=dff_f_mean[i][switch2:][X1:X2]\n",
    "            dff_average_minute_Rfamiliar_each.append(dff_minute) \n",
    "    velocity_minute_Rfamiliar += [velocity_average_minute_Rfamiliar_each]\n",
    "    dff_minute_Rfamiliar += [dff_average_minute_Rfamiliar_each]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1713be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data before filter and resample\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of rows (i) and columns (n) you want\n",
    "rows = len(velocity_familiar_all)  # Assuming i represents the number of entries in speed_familiar_all\n",
    "columns = 10  # n is set to 10 based on your description\n",
    "\n",
    "# Create a figure with a grid of subplots\n",
    "fig, axes = plt.subplots(rows, columns, figsize=(columns * 5, rows * 3))  # Adjust figsize as needed\n",
    "\n",
    "# Plot the KDEs in each subplot\n",
    "for i in range(rows):\n",
    "    for n in range(columns):\n",
    "        ax = axes[i, n]  # Get the current subplot\n",
    "        \n",
    "        # Plot the KDEs\n",
    "        sns.kdeplot(velocity_familiar_all[i], color='grey', linewidth=2, linestyle='-', label='Familiar', ax=ax)\n",
    "        sns.kdeplot(velocity_minute_novel[i][n], color='red', linewidth=2, linestyle='-', label='Novel', ax=ax)\n",
    "        sns.kdeplot(velocity_minute_Rfamiliar[i][n], color='black', linewidth=2, linestyle='-', label='Rfamiliar', ax=ax)\n",
    "        \n",
    "        # Customize the subplot appearance\n",
    "        # Remove x-ticks and y-ticks\n",
    "        ax.set_xticks([])  # Remove x-ticks\n",
    "        ax.set_yticks([])  # Remove y-ticks\n",
    "\n",
    "        # Remove axis labels\n",
    "        ax.set_xlabel('')  # Remove x-axis label\n",
    "        ax.set_ylabel('')  # Remove y-axis label\n",
    "        \n",
    "        # Remove top and right spines\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        \n",
    "        # Change border color and thickness\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color('black')\n",
    "            spine.set_linewidth(2)\n",
    "\n",
    "        # Optionally remove legend for cleaner appearance\n",
    "        # ax.legend(loc='upper right', fontsize=10, frameon=False)\n",
    "\n",
    "# Adjust layout so subplots fit nicely\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot with high resolution (DPI)\n",
    "#plt.savefig(\"unfiltered_velocity.png\", dpi=600)  # Increase DPI for higher resolution\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d4a18",
   "metadata": {},
   "source": [
    "### Wilocoxon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7472f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Convert lists to numpy arrays if not already arrays and handle None values by converting them to np.nan\n",
    "speed_average_minute_novel = np.array(filtered_resampled_dff_minute_novel, dtype=np.float64)\n",
    "speed_average_minute_Rfamiliar = np.array(filtered_resampled_dff_minute_Rfamiliar, dtype=np.float64)\n",
    "speed_average_minute_familiar = np.mean(filtered_dff_minute_ave_familiar, axis=1)\n",
    "\n",
    "# Ensure all arrays are of shape (N, 10) where N is the number of samples\n",
    "speed_average_minute_novel = speed_average_minute_novel[:, :10]\n",
    "speed_average_minute_Rfamiliar = speed_average_minute_Rfamiliar[:, :10]\n",
    "\n",
    "# Initialize lists to store p-values\n",
    "p_values_novel = []\n",
    "p_values_Rfamiliar = []\n",
    "p_values_Rnovel = []\n",
    "len_familiar=[]\n",
    "len_novel=[]\n",
    "len_Rfamiliar=[]\n",
    "excluded_familiar=[]\n",
    "excluded_novel=[]\n",
    "excluded_Rfamiliar=[]\n",
    "\n",
    "for i in range(10):\n",
    "    # Extract data for the current time point, excluding rows with NaN in any group\n",
    "    novel_time_point = speed_average_minute_novel[:, i]\n",
    "    Rfamiliar_time_point = speed_average_minute_Rfamiliar[:, i]\n",
    "    \n",
    "    # Create a mask to identify rows without NaN values across all groups for the current time point\n",
    "    valid_mask = ~np.isnan(speed_average_minute_familiar) & ~np.isnan(novel_time_point) & ~np.isnan(Rfamiliar_time_point)\n",
    "    \n",
    "    # Apply the mask to keep only rows with valid data in all groups\n",
    "    filtered_familiar = speed_average_minute_familiar[valid_mask]\n",
    "    filtered_novel = novel_time_point[valid_mask]\n",
    "    filtered_Rfamiliar = Rfamiliar_time_point[valid_mask]\n",
    "    \n",
    "    # Compare familiar vs novel\n",
    "    stat, p = wilcoxon(filtered_familiar, filtered_novel)\n",
    "    p_values_novel.append(p)\n",
    "    \n",
    "    # Compare familiar vs returned familiar\n",
    "    stat, p = wilcoxon(filtered_familiar, filtered_Rfamiliar)\n",
    "    p_values_Rfamiliar.append(p)\n",
    "    \n",
    "    stat, p = wilcoxon(filtered_novel, filtered_Rfamiliar)\n",
    "    p_values_Rnovel.append(p)\n",
    "    \n",
    "    len_familiar.append(len(filtered_familiar))\n",
    "    len_novel.append(len(filtered_novel))\n",
    "    len_Rfamiliar.append(len(filtered_Rfamiliar))\n",
    "    excluded_familiar.append(filtered_familiar.tolist())\n",
    "    excluded_novel.append(filtered_novel.tolist())\n",
    "    excluded_Rfamiliar.append(filtered_Rfamiliar.tolist())\n",
    "    \n",
    "\n",
    "# Print the p-values\n",
    "print(\"P-values for familiar vs novel at each time point:\", p_values_novel)\n",
    "print(\"P-values for familiar vs returned familiar at each time point:\", p_values_Rfamiliar)\n",
    "print(\"P-values for novel vs returned familiar at each time point:\", p_values_Rnovel)\n",
    "print('F_length', len_familiar)\n",
    "print('N_length', len_novel)\n",
    "print('R_length', len_Rfamiliar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727fa2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Convert lists to numpy arrays if not already arrays and handle None values by converting them to np.nan\n",
    "speed_average_minute_novel = np.array(filtered_resampled_dff_minute_novel, dtype=np.float64)\n",
    "speed_average_minute_Rfamiliar = np.array(filtered_resampled_dff_minute_Rfamiliar, dtype=np.float64)\n",
    "speed_average_minute_familiar = np.mean(filtered_dff_minute_ave_familiar, axis=1)\n",
    "\n",
    "# Ensure all arrays are of shape (N, 10) where N is the number of samples\n",
    "speed_average_minute_novel = speed_average_minute_novel[:, :10]\n",
    "speed_average_minute_Rfamiliar = speed_average_minute_Rfamiliar[:, :10]\n",
    "\n",
    "# Initialize lists to store p-values\n",
    "p_values_novel = []\n",
    "p_values_Rfamiliar = []\n",
    "p_values_Rnovel = []\n",
    "len_familiar=[]\n",
    "len_novel=[]\n",
    "len_Rfamiliar=[]\n",
    "\n",
    "i = 0\n",
    "    # Extract data for the current time point, excluding rows with NaN in any group\n",
    "novel_time_point = speed_average_minute_novel[1:, i] #remove C7 data\n",
    "Rfamiliar_time_point = speed_average_minute_Rfamiliar[1:, i] #remove C7 data\n",
    "\n",
    "\n",
    "    # Create a mask to identify rows without NaN values across all groups for the current time point\n",
    "valid_mask = ~np.isnan(speed_average_minute_familiar[1:]) & ~np.isnan(novel_time_point) & ~np.isnan(Rfamiliar_time_point)\n",
    "    \n",
    "    # Apply the mask to keep only rows with valid data in all groups\n",
    "filtered_familiar = speed_average_minute_familiar[1:][valid_mask]\n",
    "filtered_novel = novel_time_point[valid_mask]\n",
    "filtered_Rfamiliar = Rfamiliar_time_point[valid_mask]\n",
    "    \n",
    "    # Compare familiar vs novel\n",
    "stat, p = wilcoxon(filtered_familiar, filtered_novel)\n",
    "p_values_novel.append(p)\n",
    "    \n",
    "    # Compare familiar vs returned familiar\n",
    "stat, p = wilcoxon(filtered_familiar, filtered_Rfamiliar)\n",
    "p_values_Rfamiliar.append(p)\n",
    "    \n",
    "stat, p = wilcoxon(filtered_novel, filtered_Rfamiliar)\n",
    "p_values_Rnovel.append(p)\n",
    "    \n",
    "len_familiar.append(len(filtered_familiar))\n",
    "len_novel.append(len(filtered_novel))\n",
    "len_Rfamiliar.append(len(filtered_Rfamiliar))\n",
    "    \n",
    "\n",
    "# Print the p-values\n",
    "print(\"P-values for familiar vs novel at each time point:\", p_values_novel)\n",
    "print(\"P-values for familiar vs returned familiar at each time point:\", p_values_Rfamiliar)\n",
    "print(\"P-values for novel vs returned familiar at each time point:\", p_values_Rnovel)\n",
    "print('F_length', len_familiar)\n",
    "print('N_length', len_novel)\n",
    "print('R_length', len_Rfamiliar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f694433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Convert lists to numpy arrays if not already arrays and handle None values by converting them to np.nan\n",
    "speed_average_minute_novel = np.array(filtered_resampled_dff_minute_novel, dtype=np.float64)\n",
    "\n",
    "excluded_novel2=[]\n",
    "\n",
    "for i in range(10,20):\n",
    "    # Extract data for the current time point, excluding rows with NaN in any group\n",
    "    novel_time_point = speed_average_minute_novel[:, i]\n",
    "    \n",
    "    # Create a mask to identify rows without NaN values across all groups for the current time point\n",
    "    valid_mask = ~np.isnan(novel_time_point)\n",
    " \n",
    "    filtered_novel = novel_time_point[valid_mask]\n",
    "\n",
    "    excluded_novel2.append(filtered_novel.tolist())\n",
    "    \n",
    "# Print\n",
    "print(len(excluded_novel2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b68799",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(excluded_novel+excluded_novel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dcf81",
   "metadata": {},
   "source": [
    "## plotting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legend labels\n",
    "legend_labels = [\"novel\", \"returned familiar\"]\n",
    "\n",
    "# Prepare data\n",
    "excluded_novel20=excluded_novel+excluded_novel2 #20min data\n",
    "excluded_familiar20=excluded_familiar+excluded_familiar #20 min data\n",
    "\n",
    "increased_familiar = [[familiar - familiar for familiar, familiar in zip(row_familiar, row_familiar)]\n",
    "for row_familiar, row_familiar in zip(excluded_familiar20, excluded_familiar20)]\n",
    "increased_novel = [[novel - familiar for novel, familiar in zip(row_novel, row_familiar)]\n",
    "for row_novel, row_familiar in zip(excluded_novel20, excluded_familiar20)]\n",
    "increased_Rfamiliar = [[Rfamiliar - familiar for Rfamiliar, familiar in zip(row_Rfamiliar, row_familiar)]\n",
    "for row_Rfamiliar, row_familiar in zip(excluded_Rfamiliar, excluded_familiar)]\n",
    "\n",
    "mean_familiar = []\n",
    "mean_novel = []\n",
    "mean_Rfamiliar = []\n",
    "ste_familiar = []\n",
    "ste_novel = []\n",
    "ste_Rfamiliar = []\n",
    "\n",
    "# Assuming each increased_* is a list of lists, we need to iterate through each sublist\n",
    "for i in range(10):\n",
    "    mean_familiar.append(np.mean(increased_familiar[i]))\n",
    "    mean_Rfamiliar.append(np.mean(increased_Rfamiliar[i]))\n",
    "    ste_familiar.append(np.std(increased_familiar[i]) / np.sqrt(len(increased_familiar[i])))\n",
    "    ste_Rfamiliar.append(np.std(increased_Rfamiliar[i]) / np.sqrt(len(increased_Rfamiliar[i])))\n",
    "for i in range(20):    \n",
    "    mean_novel.append(np.mean(increased_novel[i]))\n",
    "    ste_novel.append(np.std(increased_novel[i]) / np.sqrt(len(increased_novel[i])))\n",
    "\n",
    "# Combine the filtered data lists for easier iteration\n",
    "mean_data_lists = [mean_novel, mean_Rfamiliar]\n",
    "ste_data_lists = [ste_novel, ste_Rfamiliar]\n",
    "\n",
    "# Define colors for each criterion\n",
    "colors = [\"red\", \"black\"]\n",
    "\n",
    "# Define lighter colors for filling\n",
    "lighter_colors = [\"#FFB6C1\", \"#A9A9A9\"]\n",
    "\n",
    "# Prepare figure\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Iterate over each data list to plot\n",
    "for i, (avg_data, std_error) in enumerate(zip(mean_data_lists, ste_data_lists)):\n",
    "    # Convert mean and std error lists to NumPy arrays\n",
    "    avg_data = np.array(avg_data)\n",
    "    std_error = np.array(std_error)\n",
    "\n",
    "    # Define x-values specific to the length of avg_data\n",
    "    x_values = np.arange(1, len(avg_data) + 1)\n",
    "\n",
    "    # Plot average data\n",
    "    plt.plot(x_values, avg_data, label=legend_labels[i], color=colors[i], linewidth=2.5)\n",
    "\n",
    "    # Plot standard error as filled area\n",
    "    plt.fill_between(x_values, avg_data - std_error, avg_data + std_error, color=lighter_colors[i], alpha=0.5)\n",
    "\n",
    "# Plot familiar_mean as a dashed line (without error bars)\n",
    "familiar_mean_avg = np.mean(mean_familiar)\n",
    "plt.plot(range(1, len(mean_data_lists[0]) + 1), [familiar_mean_avg] * len(mean_data_lists[0]), \n",
    "         label=\"familiar_mean\", color=\"grey\", linestyle=\"--\", linewidth=4)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('resampled_filtered_dF/F', fontsize=22)\n",
    "plt.xlabel('Time (min)', fontsize=22)\n",
    "plt.ylabel('F/F increased from familiar (%)', fontsize=22)\n",
    "\n",
    "# Set x-ticks\n",
    "plt.xticks(ticks=[0, 5, 10, 15, 20], fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "# Change the thickness of x-ticks and y-ticks\n",
    "plt.tick_params(axis='both', width=2)\n",
    "\n",
    "# Adding a legend outside the plot\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=18)\n",
    "\n",
    "# Remove top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Change border color and thickness\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('black')\n",
    "    spine.set_linewidth(2)\n",
    "\n",
    "# Layout adjustment to accommodate legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef99e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legend labels\n",
    "legend_labels = [\"novel\", \"returned familiar\"]\n",
    "\n",
    "# Prepare data\n",
    "excluded_novel20=excluded_novel+excluded_novel2 #20min data\n",
    "excluded_familiar20=excluded_familiar+excluded_familiar #20 min data\n",
    "\n",
    "mean_familiar = []\n",
    "mean_novel = []\n",
    "mean_Rfamiliar = []\n",
    "ste_familiar = []\n",
    "ste_novel = []\n",
    "ste_Rfamiliar = []\n",
    "\n",
    "# Assuming each increased_* is a list of lists, we need to iterate through each sublist\n",
    "for i in range(10):\n",
    "    mean_familiar.append(np.mean(excluded_familiar20[i]))\n",
    "    mean_Rfamiliar.append(np.mean(excluded_Rfamiliar[i]))\n",
    "    ste_familiar.append(np.std(excluded_familiar20[i]) / np.sqrt(len(excluded_familiar20[i])))\n",
    "    ste_Rfamiliar.append(np.std(excluded_Rfamiliar[i]) / np.sqrt(len(excluded_Rfamiliar[i])))\n",
    "for i in range(20):    \n",
    "    mean_novel.append(np.mean(excluded_novel20[i]))\n",
    "    ste_novel.append(np.std(excluded_novel20[i]) / np.sqrt(len(excluded_novel20[i])))\n",
    "\n",
    "# Combine the filtered data lists for easier iteration\n",
    "mean_data_lists = [mean_novel, mean_Rfamiliar]\n",
    "ste_data_lists = [ste_novel, ste_Rfamiliar]\n",
    "\n",
    "# Define colors for each criterion\n",
    "colors = [\"red\", \"black\"]\n",
    "\n",
    "# Define lighter colors for filling\n",
    "lighter_colors = [\"#FFB6C1\", \"#A9A9A9\"]\n",
    "\n",
    "# Prepare figure\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Iterate over each data list to plot\n",
    "for i, (avg_data, std_error) in enumerate(zip(mean_data_lists, ste_data_lists)):\n",
    "    # Convert mean and std error lists to NumPy arrays\n",
    "    avg_data = np.array(avg_data)\n",
    "    std_error = np.array(std_error)\n",
    "\n",
    "    # Define x-values specific to the length of avg_data\n",
    "    x_values = np.arange(1, len(avg_data) + 1)\n",
    "\n",
    "    # Plot average data\n",
    "    plt.plot(x_values, avg_data, label=legend_labels[i], color=colors[i], linewidth=2.5)\n",
    "\n",
    "    # Plot standard error as filled area\n",
    "    plt.fill_between(x_values, avg_data - std_error, avg_data + std_error, color=lighter_colors[i], alpha=0.5)\n",
    "\n",
    "# Plot familiar_mean as a dashed line (without error bars)\n",
    "familiar_mean_avg = np.mean(mean_familiar)\n",
    "plt.plot(range(1, len(mean_data_lists[0]) + 1), [familiar_mean_avg] * len(mean_data_lists[0]), \n",
    "         label=\"familiar_mean\", color=\"grey\", linestyle=\"--\", linewidth=4)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('resampled_filtered_dF/F', fontsize=22)\n",
    "plt.xlabel('Time (min)', fontsize=22)\n",
    "plt.ylabel('F/F (%)', fontsize=22)\n",
    "\n",
    "# Set x-ticks\n",
    "plt.xticks(ticks=[0, 5, 10, 15, 20], fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "# Change the thickness of x-ticks and y-ticks\n",
    "plt.tick_params(axis='both', width=2)\n",
    "\n",
    "# Adding a legend outside the plot\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=18)\n",
    "\n",
    "# Remove top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Change border color and thickness\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('black')\n",
    "    spine.set_linewidth(2)\n",
    "\n",
    "# Layout adjustment to accommodate legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43733200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff change at 1st min\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "speed_average_minute_novel = np.array(filtered_resampled_dff_minute_novel, dtype=np.float64)\n",
    "speed_average_minute_Rfamiliar = np.array(filtered_resampled_dff_minute_Rfamiliar, dtype=np.float64)\n",
    "speed_average_minute_familiar = np.mean(filtered_dff_minute_ave_familiar, axis=1)\n",
    "\n",
    "# Ensure all arrays are of shape (N, 10) where N is the number of samples\n",
    "speed_average_minute_novel = speed_average_minute_novel[:, :10]\n",
    "speed_average_minute_Rfamiliar = speed_average_minute_Rfamiliar[:, :10]\n",
    "\n",
    "i=0\n",
    "    # Extract data for the current time point, excluding rows with NaN in any group\n",
    "novel_time_point = speed_average_minute_novel[:, i] \n",
    "Rfamiliar_time_point = speed_average_minute_Rfamiliar[:, i] \n",
    "    \n",
    "    # Create a mask to identify rows without NaN values across all groups for the current time point\n",
    "valid_mask = ~np.isnan(speed_average_minute_familiar) & ~np.isnan(novel_time_point) & ~np.isnan(Rfamiliar_time_point)\n",
    "    \n",
    "    # Apply the mask to keep only rows with valid data in all groups\n",
    "filtered_familiar = speed_average_minute_familiar[valid_mask]\n",
    "filtered_novel = novel_time_point[valid_mask]\n",
    "filtered_Rfamiliar = Rfamiliar_time_point[valid_mask]\n",
    "\n",
    "novel_familiar=filtered_novel-filtered_familiar\n",
    "Rfamiliar_familiar=filtered_Rfamiliar-filtered_familiar\n",
    "familiar_familiar=filtered_familiar-filtered_familiar\n",
    "# Calculate mean values\n",
    "average_familiar = np.mean(familiar_familiar)\n",
    "average_novel = np.mean(novel_familiar)\n",
    "average_Rfamiliar = np.mean(Rfamiliar_familiar)\n",
    "\n",
    "# Prepare data for bar plot\n",
    "group_labels = ['Familiar','Novel', 'Rfamiliar']\n",
    "mean_values = [average_familiar,average_novel, average_Rfamiliar]\n",
    "\n",
    "# Adjust x-positions to give more space between the groups\n",
    "x_positions = [0.98, 1.01, 1.04]  # Reduce the distance between the two bars\n",
    "\n",
    "# Plot the bar plot, scatter plots, and dashed line\n",
    "plt.figure(figsize=(6.6, 6.6))\n",
    "for i in range(len(novel_familiar)):\n",
    "    plt.plot([x_positions[0], x_positions[1]], \n",
    "             [familiar_familiar[i], novel_familiar[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "    # Add the index label next to each point\n",
    "    #plt.text(x_positions[1] - 0.0001, xcorr_minute_novel_first_min[i], str(i+1), fontsize=12, color='black')\n",
    "    plt.plot([x_positions[1], x_positions[2]], \n",
    "             [novel_familiar[i], Rfamiliar_familiar[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "\n",
    "# Scatter plot for familiar group\n",
    "plt.scatter([x_positions[0]] * len(familiar_familiar), \n",
    "            familiar_familiar, s=100, color='grey', alpha=0.7, label='Familiar')    \n",
    "    \n",
    "# Scatter plot for Novel group\n",
    "plt.scatter([x_positions[1]] * len(novel_familiar), \n",
    "            novel_familiar, s=100, color='red', alpha=0.7, label='Novel')\n",
    "\n",
    "# Scatter plot for R-Familiar group\n",
    "plt.scatter([x_positions[2]] * len(Rfamiliar_familiar), \n",
    "            Rfamiliar_familiar, s=100, color='black', alpha=0.7, label='Rfamiliar')\n",
    "\n",
    "# Add a dashed line at the mean value for Familiar group across the entire plot\n",
    "#plt.axhline(y=average_xcorr_familiar, color='grey', linestyle='--', linewidth=2, label='Mean Familiar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('F/F increased from familiar (%)', size=21)\n",
    "plt.title('1st minute', fontsize=21, pad=10)\n",
    "\n",
    "# Set y-limits to make the plot cleaner\n",
    "plt.ylim(-1.7, 9)\n",
    "plt.xlim(0.97,1.05)\n",
    "\n",
    "# Set custom x-ticks for the two groups (Novel, Rfamiliar)\n",
    "plt.xticks(x_positions, ['Familiar(Avg)','Novel', 'Rfamiliar'], fontsize=20)\n",
    "plt.yticks([0, 2, 4, 6, 8], fontsize=20)\n",
    "\n",
    "# Move the legend outside the plot and remove border\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14, frameon=False)\n",
    "\n",
    "# Remove top and right spines (border)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Change border color and thickness\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('black')  # Change border color to black\n",
    "    spine.set_linewidth(2)     # Increase border thickness\n",
    "\n",
    "# Show plot\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b48bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dff change at 1st min with each label\n",
    "\n",
    "# Convert lists to numpy arrays \n",
    "speed_average_minute_novel = np.array(filtered_resampled_dff_minute_novel, dtype=np.float64)\n",
    "speed_average_minute_Rfamiliar = np.array(filtered_resampled_dff_minute_Rfamiliar, dtype=np.float64)\n",
    "speed_average_minute_familiar = np.mean(filtered_dff_minute_ave_familiar, axis=1)\n",
    "\n",
    "# Ensure all arrays are of shape (N, 10) where N is the number of samples\n",
    "speed_average_minute_novel = speed_average_minute_novel[:, :10]\n",
    "speed_average_minute_Rfamiliar = speed_average_minute_Rfamiliar[:, :10]\n",
    "\n",
    "i=0\n",
    "    # Extract data for the current time point, excluding rows with NaN in any group\n",
    "novel_time_point = speed_average_minute_novel[1:, i]\n",
    "Rfamiliar_time_point = speed_average_minute_Rfamiliar[1:, i]\n",
    "    \n",
    "    # Create a mask to identify rows without NaN values across all groups for the current time point\n",
    "valid_mask = ~np.isnan(speed_average_minute_familiar[1:]) & ~np.isnan(novel_time_point) & ~np.isnan(Rfamiliar_time_point)\n",
    "    \n",
    "    # Apply the mask to keep only rows with valid data in all groups\n",
    "filtered_familiar = speed_average_minute_familiar[1:][valid_mask]\n",
    "filtered_novel = novel_time_point[valid_mask]\n",
    "filtered_Rfamiliar = Rfamiliar_time_point[valid_mask]\n",
    "\n",
    "novel_familiar=filtered_novel-filtered_familiar\n",
    "Rfamiliar_familiar=filtered_Rfamiliar-filtered_familiar\n",
    "familiar_familiar=filtered_familiar-filtered_familiar\n",
    "# Calculate mean values\n",
    "average_familiar = np.mean(familiar_familiar)\n",
    "average_novel = np.mean(novel_familiar)\n",
    "average_Rfamiliar = np.mean(Rfamiliar_familiar)\n",
    "\n",
    "# Prepare data for bar plot\n",
    "group_labels = ['Familiar','Novel', 'Rfamiliar']\n",
    "mean_values = [average_familiar,average_novel, average_Rfamiliar]\n",
    "\n",
    "# Adjust x-positions to give more space between the groups\n",
    "x_positions = [0.98, 1.01, 1.04]  # Reduce the distance between the two bars\n",
    "\n",
    "# Plot the bar plot, scatter plots, and dashed line\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(len(novel_familiar)):\n",
    "    plt.plot([x_positions[0], x_positions[1]], \n",
    "             [familiar_familiar[i], novel_familiar[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "    # Add the index label next to each point\n",
    "    plt.plot([x_positions[1], x_positions[2]], \n",
    "             [novel_familiar[i], Rfamiliar_familiar[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "    \n",
    "        # Add the index label next to each point for each group\n",
    "    plt.text(x_positions[2] + 0.002, Rfamiliar_familiar[i], str(i + 1), fontsize=9, color='black')\n",
    "\n",
    "# Scatter plot for familiar group\n",
    "plt.scatter([x_positions[0]] * len(familiar_familiar), \n",
    "            familiar_familiar, s=100, color='grey', alpha=0.7, label='Familiar')\n",
    "    \n",
    "# Scatter plot for Novel group\n",
    "plt.scatter([x_positions[1]] * len(novel_familiar), \n",
    "            novel_familiar, s=100, color='red', alpha=0.7, label='Novel')\n",
    "\n",
    "# Scatter plot for R-Familiar group\n",
    "plt.scatter([x_positions[2]] * len(Rfamiliar_familiar), \n",
    "            Rfamiliar_familiar, s=100, color='black', alpha=0.7, label='Rfamiliar')\n",
    "\n",
    "# Add a dashed line at the mean value for Familiar group across the entire plot\n",
    "#plt.axhline(y=average_xcorr_familiar, color='grey', linestyle='--', linewidth=2, label='Mean Familiar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('F/F increased from familiar (%)', size=21)\n",
    "plt.title('1st minute', fontsize=21, pad=40)\n",
    "\n",
    "# Set y-limits to make the plot cleaner\n",
    "plt.ylim(-1.7, 9)\n",
    "plt.xlim(0.97,1.05)\n",
    "\n",
    "# Set custom x-ticks for the two groups (Novel, Rfamiliar)\n",
    "plt.xticks(x_positions, ['Familiar (Avg)','Novel', 'Rfamiliar'], fontsize=18)\n",
    "plt.yticks([0, 2, 4, 6, 8], fontsize=18)\n",
    "\n",
    "# Move the legend outside the plot and remove border\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14, frameon=False)\n",
    "\n",
    "# Remove top and right spines (border)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Change border color and thickness\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('black')  # Change border color to black\n",
    "    spine.set_linewidth(2)     # Increase border thickness\n",
    "\n",
    "# Show plot\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5185a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff change at 5th min\n",
    "\n",
    "# Convert lists to numpy arrays \n",
    "speed_average_minute_novel = np.array(filtered_resampled_dff_minute_novel, dtype=np.float64)\n",
    "speed_average_minute_Rfamiliar = np.array(filtered_resampled_dff_minute_Rfamiliar, dtype=np.float64)\n",
    "speed_average_minute_familiar = np.mean(filtered_dff_minute_ave_familiar, axis=1)\n",
    "\n",
    "# Ensure all arrays are of shape (N, 10) where N is the number of samples\n",
    "speed_average_minute_novel = speed_average_minute_novel[:, :10]\n",
    "speed_average_minute_Rfamiliar = speed_average_minute_Rfamiliar[:, :10]\n",
    "\n",
    "i=4\n",
    "    # Extract data for the current time point, excluding rows with NaN in any group\n",
    "novel_time_point = speed_average_minute_novel[:, i] \n",
    "Rfamiliar_time_point = speed_average_minute_Rfamiliar[:, i] \n",
    "    \n",
    "    # Create a mask to identify rows without NaN values across all groups for the current time point\n",
    "valid_mask = ~np.isnan(speed_average_minute_familiar) & ~np.isnan(novel_time_point) & ~np.isnan(Rfamiliar_time_point)\n",
    "    \n",
    "    # Apply the mask to keep only rows with valid data in all groups\n",
    "filtered_familiar = speed_average_minute_familiar[valid_mask]\n",
    "filtered_novel = novel_time_point[valid_mask]\n",
    "filtered_Rfamiliar = Rfamiliar_time_point[valid_mask]\n",
    "\n",
    "novel_familiar=filtered_novel-filtered_familiar\n",
    "Rfamiliar_familiar=filtered_Rfamiliar-filtered_familiar\n",
    "familiar_familiar=filtered_familiar-filtered_familiar\n",
    "# Calculate mean values\n",
    "average_familiar = np.mean(familiar_familiar)\n",
    "average_novel = np.mean(novel_familiar)\n",
    "average_Rfamiliar = np.mean(Rfamiliar_familiar)\n",
    "\n",
    "# Prepare data for bar plot\n",
    "group_labels = ['Familiar','Novel', 'Rfamiliar']\n",
    "mean_values = [average_familiar,average_novel, average_Rfamiliar]\n",
    "\n",
    "# Adjust x-positions to give more space between the groups\n",
    "x_positions = [0.98, 1.01, 1.04]  # Reduce the distance between the two bars\n",
    "\n",
    "# Plot the bar plot, scatter plots, and dashed line\n",
    "plt.figure(figsize=(6.6, 6.6))\n",
    "for i in range(len(novel_familiar)):\n",
    "    plt.plot([x_positions[0], x_positions[1]], \n",
    "             [familiar_familiar[i], novel_familiar[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "    # Add the index label next to each point\n",
    "    #plt.text(x_positions[1] - 0.0001, xcorr_minute_novel_first_min[i], str(i+1), fontsize=12, color='black')\n",
    "    plt.plot([x_positions[1], x_positions[2]], \n",
    "             [novel_familiar[i], Rfamiliar_familiar[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "\n",
    "# Scatter plot for familiar group\n",
    "plt.scatter([x_positions[0]] * len(familiar_familiar), \n",
    "            familiar_familiar, s=100, color='grey', alpha=0.7, label='Familiar')    \n",
    "    \n",
    "# Scatter plot for Novel group\n",
    "plt.scatter([x_positions[1]] * len(novel_familiar), \n",
    "            novel_familiar, s=100, color='red', alpha=0.7, label='Novel')\n",
    "\n",
    "# Scatter plot for R-Familiar group\n",
    "plt.scatter([x_positions[2]] * len(Rfamiliar_familiar), \n",
    "            Rfamiliar_familiar, s=100, color='black', alpha=0.7, label='Rfamiliar')\n",
    "\n",
    "# Add a dashed line at the mean value for Familiar group across the entire plot\n",
    "#plt.axhline(y=average_xcorr_familiar, color='grey', linestyle='--', linewidth=2, label='Mean Familiar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('F/F increased from familiar (%)', size=21)\n",
    "plt.title('5th  minute', fontsize=21, pad=10)\n",
    "\n",
    "# Set y-limits to make the plot cleaner\n",
    "plt.ylim(-3.2, 9)\n",
    "plt.xlim(0.97,1.05)\n",
    "\n",
    "# Set custom x-ticks for the two groups (Novel, Rfamiliar)\n",
    "plt.xticks(x_positions, ['Familiar(Avg)','Novel', 'Rfamiliar'], fontsize=20)\n",
    "plt.yticks([-2,0, 2, 4, 6, 8], fontsize=20)\n",
    "\n",
    "# Move the legend outside the plot and remove border\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14, frameon=False)\n",
    "\n",
    "# Remove top and right spines (border)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Change border color and thickness\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('black')  # Change border color to black\n",
    "    spine.set_linewidth(2)     # Increase border thickness\n",
    "\n",
    "# Show plot\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b49bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff change at 10th min\n",
    "\n",
    "# Convert lists to numpy arrays \n",
    "speed_average_minute_novel = np.array(filtered_resampled_dff_minute_novel, dtype=np.float64)\n",
    "speed_average_minute_Rfamiliar = np.array(filtered_resampled_dff_minute_Rfamiliar, dtype=np.float64)\n",
    "speed_average_minute_familiar = np.mean(filtered_dff_minute_ave_familiar, axis=1)\n",
    "\n",
    "# Ensure all arrays are of shape (N, 10) where N is the number of samples\n",
    "speed_average_minute_novel = speed_average_minute_novel[:, :10]\n",
    "speed_average_minute_Rfamiliar = speed_average_minute_Rfamiliar[:, :10]\n",
    "\n",
    "i=9\n",
    "    # Extract data for the current time point, excluding rows with NaN in any group\n",
    "novel_time_point = speed_average_minute_novel[:, i] \n",
    "Rfamiliar_time_point = speed_average_minute_Rfamiliar[:, i] \n",
    "    \n",
    "    # Create a mask to identify rows without NaN values across all groups for the current time point\n",
    "valid_mask = ~np.isnan(speed_average_minute_familiar) & ~np.isnan(novel_time_point) & ~np.isnan(Rfamiliar_time_point)\n",
    "    \n",
    "    # Apply the mask to keep only rows with valid data in all groups\n",
    "filtered_familiar = speed_average_minute_familiar[valid_mask]\n",
    "filtered_novel = novel_time_point[valid_mask]\n",
    "filtered_Rfamiliar = Rfamiliar_time_point[valid_mask]\n",
    "\n",
    "novel_familiar=filtered_novel-filtered_familiar\n",
    "Rfamiliar_familiar=filtered_Rfamiliar-filtered_familiar\n",
    "familiar_familiar=filtered_familiar-filtered_familiar\n",
    "# Calculate mean values\n",
    "average_familiar = np.mean(familiar_familiar)\n",
    "average_novel = np.mean(novel_familiar)\n",
    "average_Rfamiliar = np.mean(Rfamiliar_familiar)\n",
    "\n",
    "# Prepare data for bar plot\n",
    "group_labels = ['Familiar','Novel', 'Rfamiliar']\n",
    "mean_values = [average_familiar,average_novel, average_Rfamiliar]\n",
    "\n",
    "# Adjust x-positions to give more space between the groups\n",
    "x_positions = [0.98, 1.01, 1.04]  # Reduce the distance between the two bars\n",
    "\n",
    "# Plot the bar plot, scatter plots, and dashed line\n",
    "plt.figure(figsize=(6.6, 6.6))\n",
    "for i in range(len(novel_familiar)):\n",
    "    plt.plot([x_positions[0], x_positions[1]], \n",
    "             [familiar_familiar[i], novel_familiar[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "    # Add the index label next to each point\n",
    "    #plt.text(x_positions[1] - 0.0001, xcorr_minute_novel_first_min[i], str(i+1), fontsize=12, color='black')\n",
    "    plt.plot([x_positions[1], x_positions[2]], \n",
    "             [novel_familiar[i], Rfamiliar_familiar[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "\n",
    "# Scatter plot for familiar group\n",
    "plt.scatter([x_positions[0]] * len(familiar_familiar), \n",
    "            familiar_familiar, s=100, color='grey', alpha=0.7, label='Familiar')    \n",
    "    \n",
    "# Scatter plot for Novel group\n",
    "plt.scatter([x_positions[1]] * len(novel_familiar), \n",
    "            novel_familiar, s=100, color='red', alpha=0.7, label='Novel')\n",
    "\n",
    "# Scatter plot for R-Familiar group\n",
    "plt.scatter([x_positions[2]] * len(Rfamiliar_familiar), \n",
    "            Rfamiliar_familiar, s=100, color='black', alpha=0.7, label='Rfamiliar')\n",
    "\n",
    "# Add a dashed line at the mean value for Familiar group across the entire plot\n",
    "#plt.axhline(y=average_xcorr_familiar, color='grey', linestyle='--', linewidth=2, label='Mean Familiar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('F/F increased from familiar (%)', size=21)\n",
    "plt.title('10th  minute', fontsize=21, pad=10)\n",
    "\n",
    "# Set y-limits to make the plot cleaner\n",
    "plt.ylim(-3.2, 9)\n",
    "plt.xlim(0.97,1.05)\n",
    "\n",
    "# Set custom x-ticks for the two groups (Novel, Rfamiliar)\n",
    "plt.xticks(x_positions, ['Familiar(Avg)','Novel', 'Rfamiliar'], fontsize=20)\n",
    "plt.yticks([-2,0, 2, 4, 6, 8], fontsize=20)\n",
    "\n",
    "# Move the legend outside the plot and remove border\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14, frameon=False)\n",
    "\n",
    "# Remove top and right spines (border)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Change border color and thickness\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('black')  # Change border color to black\n",
    "    spine.set_linewidth(2)     # Increase border thickness\n",
    "\n",
    "# Show plot\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41266c0",
   "metadata": {},
   "source": [
    "# Correlation speed vs dff (w/o filter) in 0 sec gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "\n",
    "def _check_arg(x, xname):\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError('%s must be one-dimensional.' % xname)\n",
    "    return x\n",
    "\n",
    "def autocorrelation(x, maxlag):\n",
    "    \"\"\"\n",
    "    Autocorrelation with a maximum number of lags.\n",
    "\n",
    "    `x` must be a one-dimensional numpy array.\n",
    "\n",
    "    This computes the same result as\n",
    "        numpy.correlate(x, x, mode='full')[len(x)-1:len(x)+maxlag]\n",
    "\n",
    "    The return value has length maxlag + 1.\n",
    "    \"\"\"\n",
    "    x = _check_arg(x, 'x')\n",
    "    p = np.pad(x.conj(), maxlag, mode='constant')\n",
    "    T = as_strided(p[maxlag:], shape=(maxlag+1, len(x) + maxlag),\n",
    "                   strides=(-p.strides[0], p.strides[0]))\n",
    "    return T.dot(p[maxlag:].conj())\n",
    "\n",
    "\n",
    "def crosscorrelation(x, y, maxlag):\n",
    "    \"\"\"\n",
    "    Cross correlation with a maximum number of lags.\n",
    "\n",
    "    `x` and `y` must be one-dimensional numpy arrays with the same length.\n",
    "\n",
    "    This computes the same result as\n",
    "        numpy.correlate(x, y, mode='full')[len(a)-maxlag-1:len(a)+maxlag]\n",
    "\n",
    "    The return vaue has length 2*maxlag + 1.\n",
    "    \"\"\"\n",
    "    x = _check_arg(x, 'x')\n",
    "    y = _check_arg(y, 'y')\n",
    "    py = np.pad(y.conj(), 2*maxlag, mode='constant')\n",
    "    T = as_strided(py[2*maxlag:], shape=(2*maxlag+1, len(y) + 2*maxlag),\n",
    "                   strides=(-py.strides[0], py.strides[0]))\n",
    "    px = np.pad(x, maxlag, mode='constant')\n",
    "    return T.dot(px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcorr_familiar=[]\n",
    "\n",
    "for i in range(len(dff_f_mean)):    \n",
    "    \n",
    "        try:\n",
    "        # Time series data\n",
    "            switch1=switch[i].switch1.iloc[0]\n",
    "            switch2=switch[i].switch2.iloc[0]\n",
    "            data1 = dff_f_mean[i][:switch1]\n",
    "            velocity_f = savgol_filter(df_downsample[i].Velocity,50,3)\n",
    "            velocity_f=pd.DataFrame(velocity_f)\n",
    "            data2=velocity_f[0][:switch1]\n",
    "        # Detrend the data\n",
    "            detrended_data1 = detrend(data1)\n",
    "            detrended_data2 = detrend(data2)\n",
    "        # Calculate cross-correlation\n",
    "            cross_corr = crosscorrelation(detrended_data1, detrended_data2, 0) #0sec lag\n",
    "        # Normalize the cross-correlation to range from -1.0 to 1.0\n",
    "            normalized_corr = cross_corr / np.sqrt(np.sum(detrended_data1 ** 2) * np.sum(detrended_data2 ** 2))\n",
    "            xcorr_familiar.append(normalized_corr[0])\n",
    "        except:\n",
    "            print(i)\n",
    "            print('ERROR!!!')\n",
    "xcorr_familiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d441fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcorr_min_novel = []\n",
    "\n",
    "for i in range(len(dff_f_mean)):  \n",
    "    switch1 = switch[i].switch1.iloc[0]\n",
    "    switch2 = switch[i].switch2.iloc[0]\n",
    "    subtracted_time = df_downsample[i].Time[switch1:switch2] - df_downsample[i].Time[switch1]\n",
    "    velocity_f = savgol_filter(df_downsample[i].Velocity, 50, 3)\n",
    "    velocity_f = pd.DataFrame(velocity_f)\n",
    "    xcorr_min_novel_each = []\n",
    "    \n",
    "    for n in range(1,20+1):\n",
    "        if n == 1:\n",
    "            abs_diff = np.abs(subtracted_time - n * 60)\n",
    "            X = np.argmin(abs_diff)\n",
    "            data1 = dff_f_mean[i][switch1:switch2][:X]\n",
    "            data2 = velocity_f[0][switch1:switch2][:X]\n",
    "        else:\n",
    "            abs_diff1 = np.abs(subtracted_time - (n - 1) * 60)\n",
    "            X1 = np.argmin(abs_diff1)\n",
    "            abs_diff2 = np.abs(subtracted_time - n * 60)\n",
    "            X2 = np.argmin(abs_diff2)\n",
    "            data1 = dff_f_mean[i][switch1:switch2][X1:X2]\n",
    "            data2 = velocity_f[0][switch1:switch2][X1:X2]\n",
    "        \n",
    "        # Detrend the data each time\n",
    "        detrended_data1 = detrend(data1)\n",
    "        detrended_data2 = detrend(data2)\n",
    "        \n",
    "        # Calculate cross-correlation\n",
    "        cross_corr = crosscorrelation(detrended_data1, detrended_data2, 0) # 0 sec lag\n",
    "        \n",
    "        # Normalize the cross-correlation to range from -1.0 to 1.0\n",
    "        normalized_corr = cross_corr / np.sqrt(np.sum(detrended_data1 ** 2) * np.sum(detrended_data2 ** 2))\n",
    "        xcorr_min_novel_each.append(normalized_corr[0])\n",
    "    \n",
    "    xcorr_min_novel.append(xcorr_min_novel_each)\n",
    "    \n",
    "xcorr_min_novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198447dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcorr_min_Rfamiliar = []\n",
    "\n",
    "for i in range(len(dff_f_mean)):  \n",
    "    switch1 = switch[i].switch1.iloc[0]\n",
    "    switch2 = switch[i].switch2.iloc[0]\n",
    "    subtracted_time = df_downsample[i].Time[switch2:] - df_downsample[i].Time[switch2]\n",
    "    velocity_f = savgol_filter(df_downsample[i].Velocity, 50, 3)\n",
    "    velocity_f = pd.DataFrame(velocity_f)\n",
    "    xcorr_min_Rfamiliar_each = []\n",
    "    \n",
    "    for n in range(1,10+1):\n",
    "        if n == 1:\n",
    "            abs_diff = np.abs(subtracted_time - n * 60)\n",
    "            X = np.argmin(abs_diff)\n",
    "            data1 = dff_f_mean[i][switch2:][:X]\n",
    "            data2 = velocity_f[0][switch2:][:X]\n",
    "        else:\n",
    "            abs_diff1 = np.abs(subtracted_time - (n - 1) * 60)\n",
    "            X1 = np.argmin(abs_diff1)\n",
    "            abs_diff2 = np.abs(subtracted_time - n * 60)\n",
    "            X2 = np.argmin(abs_diff2)\n",
    "            data1 = dff_f_mean[i][switch2:][X1:X2]\n",
    "            data2 = velocity_f[0][switch2:][X1:X2]\n",
    "        \n",
    "        # Detrend the data each time\n",
    "        detrended_data1 = detrend(data1)\n",
    "        detrended_data2 = detrend(data2)\n",
    "        \n",
    "        # Calculate cross-correlation\n",
    "        cross_corr = crosscorrelation(detrended_data1, detrended_data2, 0) # 0 sec lag\n",
    "        \n",
    "        # Normalize the cross-correlation to range from -1.0 to 1.0\n",
    "        normalized_corr = cross_corr / np.sqrt(np.sum(detrended_data1 ** 2) * np.sum(detrended_data2 ** 2))\n",
    "        xcorr_min_Rfamiliar_each.append(normalized_corr[0])\n",
    "    \n",
    "    xcorr_min_Rfamiliar.append(xcorr_min_Rfamiliar_each)\n",
    "    \n",
    "xcorr_min_Rfamiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde80480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot R^2 over time\n",
    "\n",
    "# Legend labels\n",
    "legend_labels = [\"novel\", \"returned familiar\"]\n",
    "\n",
    "# Prepare data\n",
    "data_familiar=xcorr_familiar\n",
    "data_novel=np.array(xcorr_min_novel)\n",
    "data_Rfamiliar=np.array(xcorr_min_Rfamiliar)\n",
    "\n",
    "mean_familiar = np.mean(xcorr_familiar)\n",
    "mean_novel = []\n",
    "mean_Rfamiliar = []\n",
    "ste_novel = []\n",
    "ste_Rfamiliar = []\n",
    "\n",
    "# Assuming each increased_* is a list of lists, we need to iterate through each sublist\n",
    "for i in range(8):\n",
    "    mean_Rfamiliar.append(np.mean(data_Rfamiliar[:,i]))\n",
    "    ste_Rfamiliar.append(np.std(data_Rfamiliar[:,i]) / np.sqrt(len(data_Rfamiliar[:,i])))\n",
    "for i in range(20):    \n",
    "    mean_novel.append(np.mean(data_novel[:,i]))\n",
    "    ste_novel.append(np.std(data_novel[:,i]) / np.sqrt(len(data_novel[:,i])))\n",
    "\n",
    "# Combine the filtered data lists for easier iteration\n",
    "mean_data_lists = [mean_novel, mean_Rfamiliar]\n",
    "ste_data_lists = [ste_novel, ste_Rfamiliar]\n",
    "\n",
    "# Define colors for each criterion\n",
    "colors = [\"red\", \"black\"]\n",
    "\n",
    "# Define lighter colors for filling\n",
    "lighter_colors = [\"#FFB6C1\", \"#A9A9A9\"]\n",
    "\n",
    "# Prepare figure\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Iterate over each data list to plot\n",
    "for i, (avg_data, std_error) in enumerate(zip(mean_data_lists, ste_data_lists)):\n",
    "    # Convert mean and std error lists to NumPy arrays\n",
    "    avg_data = np.array(avg_data)\n",
    "    std_error = np.array(std_error)\n",
    "\n",
    "    # Define x-values specific to the length of avg_data\n",
    "    x_values = np.arange(1, len(avg_data) + 1)\n",
    "\n",
    "    # Plot average data\n",
    "    plt.plot(x_values, avg_data, label=legend_labels[i], color=colors[i], linewidth=2.5)\n",
    "\n",
    "    # Plot standard error as filled area\n",
    "    plt.fill_between(x_values, avg_data - std_error, avg_data + std_error, color=lighter_colors[i], alpha=0.5)\n",
    "\n",
    "# Plot familiar_mean as a dashed line (without error bars)\n",
    "familiar_mean_avg = np.mean(mean_familiar)\n",
    "plt.plot(range(1, len(mean_data_lists[0]) + 1), [familiar_mean_avg] * len(mean_data_lists[0]), \n",
    "         label=\"familiar_mean\", color=\"grey\", linestyle=\"--\", linewidth=4)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Correlation to running speed', fontsize=22)\n",
    "plt.xlabel('Time (min)', fontsize=22)\n",
    "plt.ylabel('$R^2$', fontsize=22)\n",
    "\n",
    "#plt.ylim(-0.2,-0.8)\n",
    "\n",
    "# Set x-ticks\n",
    "plt.xticks(ticks=[0, 5, 10, 15, 20], fontsize=18)\n",
    "plt.yticks([0.3,0.4,0.5,0.6,0.7], fontsize=18)\n",
    "\n",
    "# Change the thickness of x-ticks and y-ticks\n",
    "plt.tick_params(axis='both', width=2)\n",
    "\n",
    "# Adding a legend outside the plot\n",
    "#plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=18)\n",
    "\n",
    "# Remove top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Change border color and thickness\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('black')\n",
    "    spine.set_linewidth(2)\n",
    "\n",
    "# Layout adjustment to accommodate legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilocxon\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "data_familiar=xcorr_familiar\n",
    "data_novel=np.array(xcorr_min_novel)\n",
    "data_Rfamiliar=np.array(xcorr_min_Rfamiliar)\n",
    "\n",
    "# Perform Wilcoxon signed-rank test for each time point\n",
    "p_values_novel = []\n",
    "p_values_Rfamiliar = []\n",
    "p_values_Rnovel =[]\n",
    "\n",
    "for i in range(10):\n",
    "    # Compare familiar vs novel\n",
    "    stat, p = wilcoxon(data_familiar, data_novel[:, i])\n",
    "    p_values_novel.append(p)\n",
    "for i in range(8):\n",
    "    # Compare familiar vs returned familiar\n",
    "    stat, p = wilcoxon(data_familiar,data_Rfamiliar[:, i])\n",
    "    p_values_Rfamiliar.append(p)\n",
    "    # Compare novel vs returned familiar\n",
    "    stat, p = wilcoxon(data_novel[:,i],data_Rfamiliar[:, i])\n",
    "    p_values_Rnovel.append(p)\n",
    "\n",
    "# Print the p-values\n",
    "print(\"P-values for familiar vs novel at each time point:\", p_values_novel)\n",
    "print(\"P-values for familiar vs returned familiar at each time point:\", p_values_Rfamiliar)\n",
    "print(\"P-values for novel vs returned familiar at each time point:\", p_values_Rnovel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot R^2 at 1st min\n",
    "\n",
    "# Prepare data: Use the first 10 columns of each dataset\n",
    "data_familiar=xcorr_familiar\n",
    "data_novel=np.array(xcorr_min_novel)[:,0]\n",
    "data_Rfamiliar=np.array(xcorr_min_Rfamiliar)[:,0]\n",
    "\n",
    "# Calculate mean values\n",
    "average_familiar = np.mean(data_familiar)\n",
    "average_novel = np.mean(data_novel)\n",
    "average_Rfamiliar = np.mean(data_Rfamiliar)\n",
    "\n",
    "# Prepare data for bar plot\n",
    "group_labels = ['Familiar','Novel', 'Rfamiliar']\n",
    "mean_values = [average_familiar,average_novel, average_Rfamiliar]\n",
    "\n",
    "# Adjust x-positions to give more space between the groups\n",
    "x_positions = [0.98, 1.01, 1.04]  # Reduce the distance between the two bars\n",
    "\n",
    "# Plot the bar plot, scatter plots, and dashed line\n",
    "plt.figure(figsize=(6.6, 6.6))\n",
    "for i in range(len(data_novel)):\n",
    "    plt.plot([x_positions[0], x_positions[1]], \n",
    "             [data_familiar[i], data_novel[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "    plt.plot([x_positions[1], x_positions[2]], \n",
    "             [data_novel[i], data_Rfamiliar[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "\n",
    "# Scatter plot for familiar group\n",
    "plt.scatter([x_positions[0]] * len(data_familiar), \n",
    "            data_familiar, s=100, color='grey', alpha=0.7, label='Familiar')    \n",
    "    \n",
    "# Scatter plot for Novel group\n",
    "plt.scatter([x_positions[1]] * len(data_novel), \n",
    "            data_novel, s=100, color='red', alpha=0.7, label='Novel')\n",
    "\n",
    "# Scatter plot for R-Familiar group\n",
    "plt.scatter([x_positions[2]] * len(data_Rfamiliar), \n",
    "            data_Rfamiliar, s=100, color='black', alpha=0.7, label='Rfamiliar')\n",
    "\n",
    "# Add a dashed line at the mean value for Familiar group across the entire plot\n",
    "#plt.axhline(y=average_xcorr_familiar, color='grey', linestyle='--', linewidth=2, label='Mean Familiar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('$R^2$', size=21)\n",
    "plt.title('1st minute', fontsize=21, pad=20)\n",
    "\n",
    "# Set y-limits to make the plot cleaner\n",
    "plt.ylim(-0.1, 0.9)\n",
    "plt.xlim(0.97,1.05)\n",
    "\n",
    "# Set custom x-ticks for the two groups (Novel, Rfamiliar)\n",
    "plt.xticks(x_positions, ['Familiar(Avg)','  Novel', 'Rfamiliar'], fontsize=18)\n",
    "plt.yticks([0,0.2,0.4,0.6,0.8,1.0], fontsize=18)\n",
    "\n",
    "# Move the legend outside the plot and remove border\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14, frameon=False)\n",
    "\n",
    "# Remove top and right spines (border)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Change border color and thickness\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('black')  # Change border color to black\n",
    "    spine.set_linewidth(2)     # Increase border thickness\n",
    "\n",
    "# Show plot\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff35da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot R^2 at 5th min\n",
    "\n",
    "# Prepare data: Use the first 10 columns of each dataset\n",
    "data_familiar=xcorr_familiar\n",
    "data_novel=np.array(xcorr_min_novel)[:,4]\n",
    "data_Rfamiliar=np.array(xcorr_min_Rfamiliar)[:,4]\n",
    "\n",
    "# Calculate mean values\n",
    "average_familiar = np.mean(data_familiar)\n",
    "average_novel = np.mean(data_novel)\n",
    "average_Rfamiliar = np.mean(data_Rfamiliar)\n",
    "\n",
    "# Prepare data for bar plot\n",
    "group_labels = ['Familiar','Novel', 'Rfamiliar']\n",
    "mean_values = [average_familiar,average_novel, average_Rfamiliar]\n",
    "\n",
    "# Adjust x-positions to give more space between the groups\n",
    "x_positions = [0.98, 1.01, 1.04]  # Reduce the distance between the two bars\n",
    "\n",
    "# Plot the bar plot, scatter plots, and dashed line\n",
    "plt.figure(figsize=(6.6, 6.6))\n",
    "for i in range(len(data_novel)):\n",
    "    plt.plot([x_positions[0], x_positions[1]], \n",
    "             [data_familiar[i], data_novel[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "    plt.plot([x_positions[1], x_positions[2]], \n",
    "             [data_novel[i], data_Rfamiliar[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "\n",
    "# Scatter plot for familiar group\n",
    "plt.scatter([x_positions[0]] * len(data_familiar), \n",
    "            data_familiar, s=100, color='grey', alpha=0.7, label='Familiar')    \n",
    "    \n",
    "# Scatter plot for Novel group\n",
    "plt.scatter([x_positions[1]] * len(data_novel), \n",
    "            data_novel, s=100, color='red', alpha=0.7, label='Novel')\n",
    "\n",
    "# Scatter plot for R-Familiar group\n",
    "plt.scatter([x_positions[2]] * len(data_Rfamiliar), \n",
    "            data_Rfamiliar, s=100, color='black', alpha=0.7, label='Rfamiliar')\n",
    "\n",
    "# Add a dashed line at the mean value for Familiar group across the entire plot\n",
    "#plt.axhline(y=average_xcorr_familiar, color='grey', linestyle='--', linewidth=2, label='Mean Familiar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('$R^2$', size=21)\n",
    "plt.title('5th minute', fontsize=21, pad=20)\n",
    "\n",
    "# Set y-limits to make the plot cleaner\n",
    "plt.ylim(-0.1, 1)\n",
    "plt.xlim(0.97,1.05)\n",
    "\n",
    "# Set custom x-ticks for the two groups (Novel, Rfamiliar)\n",
    "plt.xticks(x_positions, ['Familiar(Avg)','  Novel', 'Rfamiliar'], fontsize=18)\n",
    "plt.yticks([0,0.2,0.4,0.6,0.8,1.0], fontsize=18)\n",
    "\n",
    "# Move the legend outside the plot and remove border\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14, frameon=False)\n",
    "\n",
    "# Remove top and right spines (border)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Change border color and thickness\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('black')  # Change border color to black\n",
    "    spine.set_linewidth(2)     # Increase border thickness\n",
    "\n",
    "# Show plot\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af926fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot R^2 at 10th min\n",
    "\n",
    "# Prepare data: Use the first 10 columns of each dataset\n",
    "data_familiar=xcorr_familiar\n",
    "data_novel=np.array(xcorr_min_novel)[:,9]\n",
    "data_Rfamiliar=np.array(xcorr_min_Rfamiliar)[:,9]\n",
    "\n",
    "# Calculate mean values\n",
    "average_familiar = np.mean(data_familiar)\n",
    "average_novel = np.mean(data_novel)\n",
    "average_Rfamiliar = np.mean(data_Rfamiliar)\n",
    "\n",
    "# Prepare data for bar plot\n",
    "group_labels = ['Familiar','Novel', 'Rfamiliar']\n",
    "mean_values = [average_familiar,average_novel, average_Rfamiliar]\n",
    "\n",
    "# Adjust x-positions to give more space between the groups\n",
    "x_positions = [0.98, 1.01, 1.04]  # Reduce the distance between the two bars\n",
    "\n",
    "# Plot the bar plot, scatter plots, and dashed line\n",
    "plt.figure(figsize=(6.6, 6.6))\n",
    "for i in range(len(data_novel)):\n",
    "    plt.plot([x_positions[0], x_positions[1]], \n",
    "             [data_familiar[i], data_novel[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "    plt.plot([x_positions[1], x_positions[2]], \n",
    "             [data_novel[i], data_Rfamiliar[i]], \n",
    "             color='grey', alpha=0.5, linewidth=2)\n",
    "\n",
    "# Scatter plot for familiar group\n",
    "plt.scatter([x_positions[0]] * len(data_familiar), \n",
    "            data_familiar, s=100, color='grey', alpha=0.7, label='Familiar')    \n",
    "    \n",
    "# Scatter plot for Novel group\n",
    "plt.scatter([x_positions[1]] * len(data_novel), \n",
    "            data_novel, s=100, color='red', alpha=0.7, label='Novel')\n",
    "\n",
    "# Scatter plot for R-Familiar group\n",
    "plt.scatter([x_positions[2]] * len(data_Rfamiliar), \n",
    "            data_Rfamiliar, s=100, color='black', alpha=0.7, label='Rfamiliar')\n",
    "\n",
    "# Add a dashed line at the mean value for Familiar group across the entire plot\n",
    "#plt.axhline(y=average_xcorr_familiar, color='grey', linestyle='--', linewidth=2, label='Mean Familiar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('$R^2$', size=21)\n",
    "plt.title('10th minute', fontsize=21, pad=20)\n",
    "\n",
    "# Set y-limits to make the plot cleaner\n",
    "plt.ylim(-0.1, 1)\n",
    "plt.xlim(0.97,1.05)\n",
    "\n",
    "# Set custom x-ticks for the two groups (Novel, Rfamiliar)\n",
    "plt.xticks(x_positions, ['Familiar(Avg)','  Novel', 'Rfamiliar'], fontsize=18)\n",
    "plt.yticks([0,0.2,0.4,0.6,0.8,1.0], fontsize=18)\n",
    "\n",
    "# Move the legend outside the plot and remove border\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14, frameon=False)\n",
    "\n",
    "# Remove top and right spines (border)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Change border color and thickness\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('black')  # Change border color to black\n",
    "    spine.set_linewidth(2)     # Increase border thickness\n",
    "\n",
    "# Show plot\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30b98b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
